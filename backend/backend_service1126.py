import json
import time
import hashlib
import threading
import os
import requests
import logging
from logging.handlers import RotatingFileHandler
from datetime import datetime, timedelta
from enum import Enum
from concurrent.futures import ThreadPoolExecutor, as_completed
from contextlib import contextmanager
import threading
from sqlalchemy import orm
from flask import Flask, jsonify, request, send_from_directory
from flask_cors import CORS
from flask_sqlalchemy import SQLAlchemy
from sqlalchemy import event, text
from config import config



# ==========================================
# 0. æ—¥å¿—é…ç½® (æ ¸å¿ƒä¼˜åŒ–)
# ==========================================

def setup_logging():
    """
    é…ç½®æ—¥å¿—ç³»ç»Ÿ
    ä¼˜åŒ–ç‚¹ï¼š
    1. ä½¿ç”¨ RotatingFileHandler é˜²æ­¢æ—¥å¿—æ–‡ä»¶æ— é™å¢é•¿
    2. ç§»é™¤ setup_thread_loggingï¼Œé¿å…å¤šçº¿ç¨‹ç¯å¢ƒä¸‹çš„å¥æŸ„æ³„éœ²
    3. ç»Ÿä¸€æ—¥å¿—æ ¼å¼ï¼Œç¡®ä¿æ‰€æœ‰çº¿ç¨‹æ—¥å¿—éƒ½è¢«æ•è·
    """
    # ç¡®ä¿æ—¥å¿—ç›®å½•å­˜åœ¨ï¼ˆé¡¹ç›®æ ¹ç›®å½•çš„logsï¼‰
    project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    log_dir = os.path.join(project_root, 'logs')
    os.makedirs(log_dir, exist_ok=True)

    # ä¸»æ—¥å¿—æ–‡ä»¶ (è‡ªåŠ¨è½®è½¬ï¼Œæœ€å¤§10MBï¼Œä¿ç•™10ä¸ªå¤‡ä»½)
    log_filename = os.path.join(log_dir, 'backend_service.log')

    # é…ç½®æ ¼å¼ï¼šå¢åŠ  [ThreadName] ä»¥ä¾¿åŒºåˆ†å¤šçº¿ç¨‹æ—¥å¿—
    log_format = config.LOG_FORMAT
    formatter = logging.Formatter(log_format)

    # è·å–æ ¹ Logger
    logger = logging.getLogger()
    logger.setLevel(getattr(logging, config.LOG_LEVEL.upper(), logging.INFO))

    # æ¸…é™¤æ—§çš„å¤„ç†å™¨ï¼Œé˜²æ­¢é‡å¤æ‰“å°
    if logger.hasHandlers():
        logger.handlers.clear()

    # 1. è½®è½¬æ–‡ä»¶å¤„ç†å™¨ (è§£å†³æ—¥å¿—è®°å½•ä¸å…¨ã€æ–‡ä»¶è¿‡å¤§çš„é—®é¢˜)
    file_handler = RotatingFileHandler(
        log_filename,
        maxBytes=10 * 1024 * 1024,  # 10MB
        backupCount=10,
        encoding='utf-8'
    )
    file_handler.setFormatter(formatter)
    file_handler.setLevel(logging.INFO)
    logger.addHandler(file_handler)

    # 2. æ§åˆ¶å°å¤„ç†å™¨
    console_handler = logging.StreamHandler()
    console_handler.setFormatter(formatter)
    console_handler.setLevel(logging.INFO)
    logger.addHandler(console_handler)

    # 3. é”™è¯¯å•ç‹¬è®°å½•
    error_log_filename = os.path.join(log_dir, 'backend_error.log')
    error_handler = RotatingFileHandler(
        error_log_filename,
        maxBytes=10 * 1024 * 1024,
        backupCount=5,
        encoding='utf-8'
    )
    error_handler.setFormatter(formatter)
    error_handler.setLevel(logging.ERROR)
    logger.addHandler(error_handler)

    return logger


# åˆå§‹åŒ–æ—¥å¿—ç³»ç»Ÿ
logger = setup_logging()

logger.info("=" * 60)
logger.info("ğŸš€ åç«¯æœåŠ¡å¯åŠ¨ - æ—¥å¿—ç³»ç»Ÿä¼˜åŒ–ç‰ˆ (Unified Logging)")
logger.info(f"ğŸ“‚ æ•°æ®åº“ç±»å‹: {config.DB_TYPE}")
logger.info("=" * 60)

# ==========================================
# 1. ä¾èµ–åº“æ£€æŸ¥ä¸åˆå§‹åŒ–
# ==========================================

try:
    import anthropic

    HAS_ANTHROPIC = True
except ImportError:
    HAS_ANTHROPIC = False
    logger.warning("âš ï¸ [æç¤º] æœªæ£€æµ‹åˆ° anthropicï¼ŒçœŸå®AIè¯„æµ‹å°†è·³è¿‡ã€‚")

try:
    from advanced_document_parser import AdvancedDocumentProcessor

    doc_parser = AdvancedDocumentProcessor(ocr_lang='chi_sim+eng')
    HAS_PARSER = True
except ImportError:
    HAS_PARSER = False
    logger.warning("âš ï¸ [æç¤º] æœªæ£€æµ‹åˆ° advanced_document_parserï¼Œå°†ä½¿ç”¨Mockè§£æã€‚")

try:
    import pandas as pd

    HAS_PANDAS = True
except ImportError:
    HAS_PANDAS = False
    logger.warning("âš ï¸ [æç¤º] æœªæ£€æµ‹åˆ° pandasï¼ŒExcel è§„åˆ™è¡¨ä»…ä½œä¸ºå…œåº•ã€‚")

app = Flask(__name__)

# ==========================================
# 1. å…¨å±€å¹¶å‘ä»»åŠ¡é…ç½®
# ==========================================

# å…¨å±€å¹¶å‘ä»»åŠ¡é…ç½®
GLOBAL_CONFIG = {
    'MAX_CONCURRENT_TASKS': int(os.getenv('MAX_CONCURRENT_TASKS', 3)),  # ä»ç¯å¢ƒå˜é‡è¯»å–ï¼Œé»˜è®¤3ä¸ª
    'RUNNING_STATES': ['SYNCING', 'EVALUATING', 'PENDING'],  # è¿è¡Œä¸­çš„çŠ¶æ€
    'RERUNNABLE_STATES': ['COMPLETED', 'ERROR', 'IDLE']     # å¯é‡æ–°è¿è¡Œçš„çŠ¶æ€
}

# å…¨å±€é”ï¼Œç”¨äºå¹¶å‘æ§åˆ¶
task_management_lock = threading.Lock()

def get_running_tasks_count():
    """
    è·å–å½“å‰æ­£åœ¨è¿è¡Œçš„ä»»åŠ¡æ•°é‡
    æ³¨æ„ï¼šè¿™ä¸ªå‡½æ•°éœ€è¦åœ¨æœ‰æ•°æ®åº“sessionçš„ä¸Šä¸‹æ–‡ä¸­è°ƒç”¨
    """
    try:
        # ä½¿ç”¨æ­£ç¡®çš„æ¨¡å‹åç§° ProjectEvaluation
        running_count = ProjectEvaluation.query.filter(
            ProjectEvaluation.status.in_(GLOBAL_CONFIG['RUNNING_STATES'])
        ).count()
        logger.info(f"å½“å‰è¿è¡Œä»»åŠ¡æ•°é‡: {running_count}")
        return running_count
    except Exception as e:
        logger.error(f"è·å–è¿è¡Œä»»åŠ¡æ•°é‡å¤±è´¥: {e}")
        return 0

def check_task_concurrency_limit():
    """
    æ£€æŸ¥æ˜¯å¦è¾¾åˆ°å¹¶å‘ä»»åŠ¡é™åˆ¶
    è¿”å›: (is_allowed, current_count, max_count, running_tasks)
    """
    try:
        current_count = get_running_tasks_count()
        max_count = GLOBAL_CONFIG['MAX_CONCURRENT_TASKS']

        # è·å–å½“å‰è¿è¡Œçš„ä»»åŠ¡åˆ—è¡¨
        running_tasks = ProjectEvaluation.query.filter(
            ProjectEvaluation.status.in_(GLOBAL_CONFIG['RUNNING_STATES'])
        ).all()

        running_task_info = [
            {
                'task_id': task.task_id,
                'project_id': task.project_id,
                'status': task.status,
                'created_at': task.created_at.isoformat() if task.created_at else None
            }
            for task in running_tasks
        ]

        is_allowed = current_count < max_count

        result = {
            'is_allowed': is_allowed,
            'current_count': current_count,
            'max_count': max_count,
            'available_slots': max_count - current_count,
            'running_tasks': running_task_info
        }

        logger.info(f"å¹¶å‘çŠ¶æ€æ£€æŸ¥ç»“æœ: {result}")
        return result

    except Exception as e:
        logger.error(f"æ£€æŸ¥å¹¶å‘ä»»åŠ¡é™åˆ¶å¤±è´¥: {e}")
        return {
            'is_allowed': False,
            'current_count': 0,
            'max_count': GLOBAL_CONFIG['MAX_CONCURRENT_TASKS'],
            'available_slots': 0,
            'running_tasks': [],
            'error': str(e)
        }

# ==========================================
# 2. æ•°æ®åº“é…ç½® (ä» config è¯»å–)
# ==========================================

# ç›´æ¥ä½¿ç”¨ Config ä¸­ç”Ÿæˆçš„ URI
app.config['SQLALCHEMY_DATABASE_URI'] = config.SQLALCHEMY_DATABASE_URI
app.config['SQLALCHEMY_TRACK_MODIFICATIONS'] = False

# é’ˆå¯¹ä¸åŒæ•°æ®åº“çš„å¼•æ“å‚æ•°ä¼˜åŒ–
engine_options = {"connect_args": {}}

if config.DB_TYPE == 'sqlite':
    # SQLite ç‰¹æœ‰è¶…æ—¶è®¾ç½®
    engine_options["connect_args"]["timeout"] = 30
    engine_options["pool_pre_ping"] = True
    engine_options["pool_recycle"] = 3600
elif config.DB_TYPE == 'mysql':
    # MySQL è¿æ¥å›æ”¶æ—¶é—´
    app.config['SQLALCHEMY_POOL_RECYCLE'] = 280
    app.config['SQLALCHEMY_POOL_SIZE'] = 10
    app.config['SQLALCHEMY_MAX_OVERFLOW'] = 20
    # æ·»åŠ å­—ç¬¦é›†è®¾ç½®ä»¥é˜²æ­¢ç¼–ç é—®é¢˜
    db_uri = config.SQLALCHEMY_DATABASE_URI
    if '?' in db_uri:
        # å¦‚æœURLå·²æœ‰å‚æ•°ï¼Œæ·»åŠ å­—ç¬¦é›†å‚æ•°
        if 'charset=' not in db_uri:
            app.config['SQLALCHEMY_DATABASE_URI'] = db_uri + '&charset=utf8mb4'
    else:
        # å¦‚æœURLæ²¡æœ‰å‚æ•°ï¼Œæ·»åŠ å­—ç¬¦é›†å‚æ•°
        app.config['SQLALCHEMY_DATABASE_URI'] = db_uri + '?charset=utf8mb4'
    
    # ç¡®ä¿è¿æ¥ä½¿ç”¨UTF8MB4å­—ç¬¦é›†
    engine_options["connect_args"] = {
        'charset': 'utf8mb4',
        'use_unicode': True
    }
    engine_options["pool_recycle"] = 280
    engine_options["pool_size"] = 10
    engine_options["max_overflow"] = 20

elif config.DB_TYPE == 'mssql':
    # SQL Server é…ç½®
    app.config['SQLALCHEMY_POOL_RECYCLE'] = 3600
    app.config['SQLALCHEMY_POOL_SIZE'] = 5
    app.config['SQLALCHEMY_MAX_OVERFLOW'] = 10
    app.config['SQLALCHEMY_POOL_PRE_PING'] = True

    # SQL Server ç‰¹å®šé…ç½® - ä¿®å¤NVARCHAR(max)ç²¾åº¦é—®é¢˜
    engine_options["connect_args"] = {
        'timeout': 30,
        'TrustServerCertificate': 'yes',
        'autocommit': True,
        'ansi': True,
        'use_native_datetime': True
    }
    engine_options["pool_recycle"] = 3600
    engine_options["pool_size"] = 5
    engine_options["max_overflow"] = 10
    # ä¿®å¤NVARCHAR(max)é—®é¢˜çš„SQLAlchemyé…ç½®
    engine_options["echo"] = False  # å…³é—­SQLæ—¥å¿—ä»¥é¿å…ç²¾åº¦é—®é¢˜
    engine_options["max_identifier_length"] = 128  # é™åˆ¶æ ‡è¯†ç¬¦é•¿åº¦
    # ç¦ç”¨ OUTPUT å­å¥ï¼Œé¿å…ä¸è§¦å‘å™¨å†²çª
    engine_options["use_insertmanyvalues"] = False

app.config['SQLALCHEMY_ENGINE_OPTIONS'] = engine_options

db = SQLAlchemy(app)

# ä»… SQLite éœ€è¦å¼€å¯ WAL æ¨¡å¼
if config.DB_TYPE == 'sqlite':
    with app.app_context():
        try:
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼Œé¿å…åˆæ¬¡è¿è¡ŒæŠ¥é”™
            if 'sqlite' in config.SQLALCHEMY_DATABASE_URI:
                db_path = config.SQLALCHEMY_DATABASE_URI.replace('sqlite:///', '')
                if not os.path.exists(db_path):
                    logger.info("åˆ›å»ºæ–°çš„ SQLite æ•°æ®åº“...")
                    db.create_all()


            @event.listens_for(db.engine, "connect")
            def set_sqlite_pragma(dbapi_connection, connection_record):
                cursor = dbapi_connection.cursor()
                cursor.execute("PRAGMA journal_mode=WAL")
                cursor.close()
        except Exception as e:
            logger.error(f"SQLite WAL é…ç½®å¤±è´¥: {e}")
else:
    # ç”Ÿäº§æ•°æ®åº“ä¸éœ€è¦æ‰‹åŠ¨å»ºè¡¨ï¼Œä½†å¼€å‘ç¯å¢ƒä¸ºäº†æ–¹ä¾¿ä»ä¿ç•™
    with app.app_context():
        try:
            db.create_all()
            logger.info(f"âœ… æ•°æ®åº“è¿æ¥æˆåŠŸ: {config.DB_TYPE}")
        except Exception as e:
            logger.error(f"âŒ æ•°æ®åº“è¿æ¥å¤±è´¥: {e}")

# çº¿ç¨‹æ± 
doc_parser_executor = ThreadPoolExecutor(
    max_workers=config.MAX_CONCURRENT_PROJECTS,
    thread_name_prefix="DocParser"
)


# ==========================================
# 3. æ•°æ®åº“æ¨¡å‹ - é€‚é…å¤šç§æ•°æ®åº“ç±»å‹
# ==========================================

# æ ¹æ®æ•°æ®åº“ç±»å‹é€‰æ‹©åˆé€‚çš„å­—ç¬¦ä¸²ç±»å‹
def get_string_field(length):
    """æ ¹æ®æ•°æ®åº“ç±»å‹è¿”å›åˆé€‚çš„å­—ç¬¦ä¸²å­—æ®µç±»å‹"""
    if config.DB_TYPE == 'mssql':
        # SQL Server ä½¿ç”¨ NVARCHAR æ”¯æŒä¸­æ–‡
        return db.NVARCHAR(length)
    else:
        # MySQL, PostgreSQL, SQLite ä½¿ç”¨ String
        return db.String(length)

def get_text_field():
    """æ ¹æ®æ•°æ®åº“ç±»å‹è¿”å›åˆé€‚çš„æ–‡æœ¬å­—æ®µç±»å‹"""
    if config.DB_TYPE == 'mssql':
        # ODBC Driver 13 å¯¹ NVARCHAR(max) æ”¯æŒæœ‰é™ï¼Œä½¿ç”¨å›ºå®šé•¿åº¦
        return db.NVARCHAR(4000)
    else:
        # å…¶ä»–æ•°æ®åº“ä½¿ç”¨ Text
        return db.Text

def safe_datetime_format(dt, format_str='%Y-%m-%d %H:%M:%S', default=''):
    """å®‰å…¨çš„datetimeæ ¼å¼åŒ–å‡½æ•°ï¼Œå¤„ç†å„ç§æ•°æ®ç±»å‹åŒ…æ‹¬datetime2"""
    if dt is None:
        return default

    try:
        if isinstance(dt, str):
            # å¦‚æœå·²ç»æ˜¯å­—ç¬¦ä¸²ï¼Œç›´æ¥è¿”å›
            # å¤„ç†datetime2å­—ç¬¦ä¸²æ ¼å¼ï¼Œå»æ‰å¾®ç§’éƒ¨åˆ†
            if '.' in dt:
                # å¤„ç† "2025-12-09 16:02:46.5366667" æ ¼å¼
                date_part = dt.split('.')[0]
                return date_part
            return dt
        elif hasattr(dt, 'strftime'):
            # å¦‚æœæ˜¯datetimeå¯¹è±¡ï¼Œè¿›è¡Œæ ¼å¼åŒ–
            # å¯¹äºdatetime2å¯¹è±¡ï¼Œå¯èƒ½éœ€è¦ç‰¹æ®Šå¤„ç†
            try:
                formatted = dt.strftime(format_str)
                return formatted
            except (ValueError, OSError, AttributeError) as format_error:
                # å¦‚æœstrftimeå¤±è´¥ï¼Œå°è¯•å…¶ä»–æ–¹æ³•
                logger.warning(f"strftimeå¤±è´¥ï¼Œå°è¯•æ›¿ä»£æ–¹æ³•: {format_error}")
                # å°è¯•è½¬æ¢ä¸ºå­—ç¬¦ä¸²å†å¤„ç†
                dt_str = str(dt)
                if '.' in dt_str:
                    return dt_str.split('.')[0]
                return dt_str
        else:
            # å…¶ä»–æƒ…å†µï¼Œå°è¯•è½¬æ¢ä¸ºå­—ç¬¦ä¸²
            dt_str = str(dt)
            # å¤„ç†å¯èƒ½çš„datetime2å­—ç¬¦ä¸²æ ¼å¼
            if '.' in dt_str:
                return dt_str.split('.')[0]
            return dt_str
    except Exception as e:
        logger.warning(f"æ—¥æœŸæ ¼å¼åŒ–å¤±è´¥: {e}, åŸå§‹æ•°æ®ç±»å‹: {type(dt)}, åŸå§‹å€¼: {dt}")
        return default

class Project(db.Model):
    # æ ¹æ®æ•°æ®åº“ç±»å‹è°ƒæ•´å­—æ®µç±»å‹
    id = db.Column(get_string_field(100), primary_key=True)
    project_code = db.Column(get_string_field(100))
    project_name = db.Column(get_string_field(255))
    # æ–°å¢é¡¹ç›®ç»ç†ç›¸å…³å­—æ®µ
    epc_manager = db.Column(get_string_field(100))  # é¡¹ç›®ç»ç†
    entrust_manager = db.Column(get_string_field(100))  # é¡¹ç›®æ‰§è¡Œç»ç†
    # ç§»é™¤ status, rules_config, evaluation_result å­—æ®µï¼Œè¿™äº›ç°åœ¨åœ¨ ProjectEvaluation ä¸­ç®¡ç†
    last_update = db.Column(db.DateTime, default=datetime.now)


class ProjectEvaluation(db.Model):
    """é¡¹ç›®è¯„æµ‹è¡¨ - æ”¯æŒåŒä¸€é¡¹ç›®å¤šä¸ªä»»åŠ¡IDçš„ç‹¬ç«‹è¯„æµ‹"""
    __tablename__ = 'project_evaluation'

    id = db.Column(db.Integer, primary_key=True, autoincrement=True)
    project_id = db.Column(get_string_field(100), db.ForeignKey('project.id'), nullable=False, index=True)
    task_id = db.Column(get_string_field(100), nullable=True, index=True)  # å¯ä»¥ä¸ºç©ºï¼Œå…¼å®¹æ—§æ•°æ®
    status = db.Column(get_string_field(50), default='IDLE')
    rules_config = db.Column(get_text_field(), nullable=True)
    evaluation_result = db.Column(get_text_field(), nullable=True)

    # æ–°å¢å­—æ®µï¼šå­˜å‚¨æ¥è‡ªæ–‡ä»¶ä¿¡æ¯æ¥å£çš„çœŸå®æ•°æ®
    check_date = db.Column(get_string_field(20), nullable=True)  # æ£€æŸ¥æ—¥æœŸï¼Œæ ¼å¼: YYYY-MM-DD
    check_person_name = db.Column(get_string_field(100), nullable=True)  # æ£€æŸ¥äººå‘˜å§“å
    check_name = db.Column(get_string_field(100), nullable=True)  # æ£€æŸ¥äººå‘˜å§“åï¼Œæ¥è‡ªä»»åŠ¡ä¿¡æ¯

    created_at = db.Column(db.DateTime, default=datetime.now)
    updated_at = db.Column(db.DateTime, default=datetime.now, onupdate=datetime.now)

    # æ·»åŠ å”¯ä¸€çº¦æŸï¼šåŒä¸€é¡¹ç›®åŒä¸€ä»»åŠ¡IDåªèƒ½æœ‰ä¸€æ¡è®°å½•
    __table_args__ = (
        db.UniqueConstraint('project_id', 'task_id', name='unique_project_task'),
        {'implicit_returning': False}  # ç¦ç”¨éšå¼ RETURNINGï¼Œé¿å… OUTPUT å­å¥
    )


class ProjectFile(db.Model):
    __tablename__ = 'project_file'

    id = db.Column(db.Integer, primary_key=True, autoincrement=True)
    project_id = db.Column(get_string_field(100), db.ForeignKey('project.id'), index=True)
    task_id = db.Column(get_string_field(100), nullable=True, index=True)  # æ–°å¢ï¼šæ”¯æŒä»»åŠ¡IDéš”ç¦»
    category_id = db.Column(get_string_field(100))
    category_name = db.Column(get_string_field(255))
    file_name = db.Column(get_string_field(255))
    file_url = db.Column(get_string_field(1000))  # URLå¯èƒ½å¾ˆé•¿
    file_type = db.Column(get_string_field(50))
    file_hash = db.Column(get_string_field(64))
    # ä½¿ç”¨é€‚é…å¤šæ•°æ®åº“çš„æ–‡æœ¬å­—æ®µç±»å‹
    parsed_content = db.Column(get_text_field(), nullable=True)  # å­˜å‚¨å¤§æ–‡æœ¬
    update_time = db.Column(db.DateTime, default=datetime.now)

    # SQL Server ç‰¹å®šé…ç½®ï¼šç¦ç”¨éšå¼ RETURNINGï¼Œé¿å… OUTPUT å­å¥ä¸è§¦å‘å™¨å†²çª
    __table_args__ = {'implicit_returning': False}


# æ•°æ®åº“å­—æ®µè‡ªæ„ˆæ£€æµ‹ (ä¸»è¦é’ˆå¯¹ SQLite/MySQL è¿ç§»æ—¶çš„å­—æ®µç¼ºå¤±)
with app.app_context():
    try:
        db.create_all()

        # æ›´æ™ºèƒ½çš„å­—æ®µæ£€æŸ¥ - æ£€æŸ¥è¡¨ç»“æ„è€Œä¸æ˜¯æŸ¥è¯¢
        with db.engine.connect() as conn:
            if config.DB_TYPE == 'mssql':
                # SQL Server æ£€æŸ¥å­—æ®µæ˜¯å¦å­˜åœ¨
                check_result = conn.execute(text("""
                    SELECT COUNT(*) as count
                    FROM INFORMATION_SCHEMA.COLUMNS
                    WHERE TABLE_NAME = 'project' AND COLUMN_NAME = 'rules_config'
                """))
            else:
                # å…¶ä»–æ•°æ®åº“çš„é€šç”¨æ£€æŸ¥
                check_result = conn.execute(text("""
                    PRAGMA table_info(project)
                """))

            field_exists = False
            if config.DB_TYPE == 'mssql':
                field_exists = check_result.fetchone()[0] > 0
            else:
                field_exists = any(row[1] == 'rules_config' for row in check_result.fetchall())

            if not field_exists:
                logger.warning("âš ï¸ rules_config å­—æ®µä¸å­˜åœ¨ï¼Œæ­£åœ¨æ·»åŠ ...")
                if config.DB_TYPE == 'mssql':
                    conn.execute(text("ALTER TABLE project ADD rules_config TEXT NULL"))
                else:
                    conn.execute(text("ALTER TABLE project ADD COLUMN rules_config TEXT"))
                logger.info("âœ… è‡ªåŠ¨ä¿®å¤: æ·»åŠ  rules_config å­—æ®µ")
            else:
                logger.info("âœ… rules_config å­—æ®µå·²å­˜åœ¨ï¼Œè·³è¿‡æ·»åŠ ")

    except Exception as e:
        logger.warning(f"âš ï¸ æ•°æ®åº“æ£€æŸ¥æ—¶å‡ºç°é—®é¢˜: {e}")
        logger.info("â„¹ï¸ è¿™é€šå¸¸ä¸æ˜¯ä¸¥é‡é—®é¢˜ï¼ŒæœåŠ¡å°†æ­£å¸¸è¿è¡Œ")
    #
    # # æ£€æŸ¥ parsed_content å­—æ®µæ˜¯å¦éœ€è¦ä¿®æ”¹ä¸º LONGTEXT (MySQL)
    # if config.DB_TYPE == 'mysql':
    #     try:
    #         with db.engine.connect() as conn:
    #             # æ£€æŸ¥å­—æ®µç±»å‹
    #             result = conn.execute(text("SHOW COLUMNS FROM project_file WHERE Field = 'parsed_content'"))
    #             column_info = result.fetchone()
    #             if column_info and 'longtext' not in column_info[1].lower():
    #                 logger.warning(f"âš ï¸ parsed_content å­—æ®µç±»å‹ä¸º {column_info[1]}ï¼Œå¯èƒ½éœ€è¦ä¿®æ”¹ä¸º LONGTEXT")
    #                 try:
    #                     # ä¿®æ”¹å­—æ®µç±»å‹ä¸º LONGTEXT
    #                     conn.execute(text("ALTER TABLE project_file MODIFY COLUMN parsed_content LONGTEXT"))
    #                     logger.info("âœ… è‡ªåŠ¨ä¿®å¤: å°† parsed_content å­—æ®µä¿®æ”¹ä¸º LONGTEXT")
    #                 except Exception as ex:
    #                     logger.error(f"âŒ ä¿®æ”¹ parsed_content å­—æ®µç±»å‹å¤±è´¥: {ex}")
    #     except Exception as e:
    #         logger.warning(f"âš ï¸ æ£€æŸ¥ parsed_content å­—æ®µç±»å‹å¤±è´¥: {e}")


# ==========================================
# 3.5. æ™ºèƒ½ä»»åŠ¡æ¢å¤ç®¡ç†å™¨
# ==========================================

class TaskStatus(Enum):
    """ä»»åŠ¡çŠ¶æ€æšä¸¾"""
    IDLE = "IDLE"
    PENDING = "PENDING"
    SYNCING = "SYNCING"
    EVALUATING = "EVALUATING"
    COMPLETED = "COMPLETED"
    ERROR = "ERROR"
    CANCELLED = "CANCELLED"
    PAUSED = "PAUSED"

class TaskRecoveryManager:
    """æ™ºèƒ½ä»»åŠ¡æ¢å¤ç®¡ç†å™¨

    æ›¿æ¢ç®€å•ç²—æš´çš„ reset_stuck_tasks å‡½æ•°ï¼Œå®ç°ï¼š
    1. åŸºäºæ—¶é—´çš„æ™ºèƒ½ä»»åŠ¡æ¢å¤ç­–ç•¥
    2. åŒºåˆ†å¯æ¢å¤å’Œä¸å¯æ¢å¤ä»»åŠ¡çŠ¶æ€
    3. è¯¦ç»†çš„é”™è¯¯è®°å½•å’Œæ¢å¤æ—¥å¿—
    4. æ”¯æŒä»»åŠ¡è‡ªæ„ˆå’Œäººå·¥å¹²é¢„
    """

    def __init__(self):
        self.recovery_threshold_hours = 1  # ä»»åŠ¡å¡ä½è¶…è¿‡2å°æ—¶æ‰è®¤ä¸ºæ˜¯éœ€è¦æ¢å¤çš„ä»»åŠ¡
        self.max_recovery_attempts = 3  # æœ€å¤§æ¢å¤å°è¯•æ¬¡æ•°

    def recover_stuck_tasks(self):
        """æ™ºèƒ½æ¢å¤å¡ä½çš„ä»»åŠ¡

        Returns:
            dict: æ¢å¤ç»“æœç»Ÿè®¡
        """
        logger.info("ğŸ”§ å¼€å§‹æ™ºèƒ½ä»»åŠ¡æ¢å¤æ£€æµ‹...")
        recovery_stats = {
            'total_checked': 0,
            'recoverable_tasks': 0,
            'recovered_tasks': 0,
            'failed_tasks': 0,
            'ignored_tasks': 0
        }

        with app.app_context():
            try:
                # æŸ¥æ‰¾å¯èƒ½å¡ä½çš„ä»»åŠ¡
                threshold = datetime.now() - timedelta(hours=self.recovery_threshold_hours)
                stuck_tasks = ProjectEvaluation.query.filter(
                    ProjectEvaluation.status.in_(['SYNCING', 'EVALUATING']),
                    ProjectEvaluation.created_at < threshold
                ).all()

                recovery_stats['total_checked'] = len(stuck_tasks)

                if not stuck_tasks:
                    logger.info("âœ… æœªå‘ç°éœ€è¦æ¢å¤çš„å¡ä½ä»»åŠ¡")
                    return recovery_stats

                logger.info(f"ğŸ” å‘ç° {len(stuck_tasks)} ä¸ªå¯èƒ½å¡ä½çš„ä»»åŠ¡")

                for task in stuck_tasks:
                    try:
                        result = self._attempt_recovery(task)
                        if result['action'] == 'recovered':
                            recovery_stats['recovered_tasks'] += 1
                        elif result['action'] == 'failed':
                            recovery_stats['failed_tasks'] += 1
                        elif result['action'] == 'ignored':
                            recovery_stats['ignored_tasks'] += 1

                        if result['is_recoverable']:
                            recovery_stats['recoverable_tasks'] += 1

                    except Exception as e:
                        logger.error(f"æ¢å¤ä»»åŠ¡å¤±è´¥ {task.project_id}:{task.task_id} - {e}")
                        recovery_stats['failed_tasks'] += 1

                db.session.commit()

                logger.info(f"ğŸ“Š ä»»åŠ¡æ¢å¤å®Œæˆç»Ÿè®¡: {recovery_stats}")

            except Exception as e:
                logger.error(f"ä»»åŠ¡æ¢å¤è¿‡ç¨‹å‘ç”Ÿé”™è¯¯: {e}")
                db.session.rollback()

        return recovery_stats

    def _attempt_recovery(self, task):
        """å°è¯•æ¢å¤å•ä¸ªä»»åŠ¡

        Args:
            task: ProjectEvaluation å®ä¾‹

        Returns:
            dict: æ¢å¤ç»“æœ
        """
        task_duration = datetime.now() - task.created_at
        duration_hours = task_duration.total_seconds() / 3600

        # è®°å½•ä¸­æ–­ä¿¡æ¯
        error_info = {
            "reason": "æœåŠ¡é‡å¯ï¼Œä»»åŠ¡è¢«ä¸­æ–­",
            "original_status": task.status,
            "interrupted_at": datetime.now().isoformat(),
            "duration_hours": round(duration_hours, 2),
            "is_recoverable": self._is_recoverable(task),
            "recovery_strategy": self._determine_recovery_strategy(task)
        }

        logger.info(f"ğŸ”§ åˆ†æä»»åŠ¡ {task.project_id}:{task.task_id} - "
                   f"çŠ¶æ€: {task.status}, æŒç»­: {duration_hours:.1f}å°æ—¶")

        if error_info["is_recoverable"]:
            # å¯æ¢å¤çš„ä»»åŠ¡ï¼šåˆ›å»ºæ¢å¤ä»»åŠ¡è®°å½•
            recovery_strategy = error_info["recovery_strategy"]

            if recovery_strategy == "auto_resume":
                # è‡ªåŠ¨æ¢å¤ï¼šé‡æ–°å¯åŠ¨ä»»åŠ¡
                return self._auto_resume_task(task, error_info)
            elif recovery_strategy == "mark_retry":
                # æ ‡è®°ä¸ºå¯é‡è¯•
                return self._mark_for_retry(task, error_info)
            else:
                # æ ‡è®°ä¸ºé”™è¯¯ä½†ä¿ç•™è¯¦ç»†ä¿¡æ¯
                return self._mark_as_error_with_info(task, error_info)
        else:
            # ä¸å¯æ¢å¤çš„ä»»åŠ¡ï¼šç›´æ¥æ ‡è®°ä¸ºé”™è¯¯
            return self._mark_as_unrecoverable(task, error_info)

    def _is_recoverable(self, task):
        """åˆ¤æ–­ä»»åŠ¡æ˜¯å¦å¯æ¢å¤

        Args:
            task: ProjectEvaluation å®ä¾‹

        Returns:
            bool: æ˜¯å¦å¯æ¢å¤
        """
        # åŸºäºä»»åŠ¡çŠ¶æ€å’ŒæŒç»­æ—¶é—´åˆ¤æ–­
        task_duration = datetime.now() - task.created_at
        duration_hours = task_duration.total_seconds() / 3600

        # è¶…è¿‡24å°æ—¶çš„ä»»åŠ¡ä¸è¿›è¡Œè‡ªåŠ¨æ¢å¤
        if duration_hours > 24:
            return False

        # æ£€æŸ¥ä¹‹å‰çš„æ¢å¤æ¬¡æ•°
        recovery_count = self._get_recovery_count(task)
        if recovery_count >= self.max_recovery_attempts:
            logger.warning(f"ä»»åŠ¡ {task.project_id}:{task.task_id} æ¢å¤æ¬¡æ•°å·²è¾¾ä¸Šé™")
            return False

        # åŸºäºä»»åŠ¡çŠ¶æ€çš„æ¢å¤ç­–ç•¥
        if task.status == 'SYNCING':
            # åŒæ­¥é˜¶æ®µçš„ä»»åŠ¡é€šå¸¸å¯ä»¥å®‰å…¨æ¢å¤
            return True
        elif task.status == 'EVALUATING':
            # è¯„æµ‹é˜¶æ®µçš„ä»»åŠ¡éœ€è¦æ›´è°¨æ…
            # æ£€æŸ¥æ˜¯å¦æœ‰éƒ¨åˆ†ç»“æœ
            if task.evaluation_result:
                try:
                    result_data = json.loads(task.evaluation_result)
                    if len(result_data) > 0:
                        logger.info(f"ä»»åŠ¡ {task.project_id}:{task.task_id} æœ‰éƒ¨åˆ†ç»“æœï¼Œæ ‡è®°ä¸ºå¯é‡è¯•")
                        return True
                except:
                    pass
            return True

        return False

    def _determine_recovery_strategy(self, task):
        """ç¡®å®šæ¢å¤ç­–ç•¥

        Args:
            task: ProjectEvaluation å®ä¾‹

        Returns:
            str: æ¢å¤ç­–ç•¥ (auto_resume, mark_retry, mark_error)
        """
        task_duration = datetime.now() - task.created_at
        duration_hours = task_duration.total_seconds() / 3600
        recovery_count = self._get_recovery_count(task)

        # é¦–æ¬¡æ¢å¤ä¸”æŒç»­æ—¶é—´è¾ƒçŸ­ï¼ˆ<6å°æ—¶ï¼‰ï¼šå°è¯•è‡ªåŠ¨æ¢å¤
        if recovery_count == 0 and duration_hours < 6:
            return "auto_resume"

        # å¤šæ¬¡æ¢å¤æˆ–æŒç»­æ—¶é—´è¾ƒé•¿ï¼šæ ‡è®°ä¸ºå¯é‡è¯•
        if recovery_count < self.max_recovery_attempts:
            return "mark_retry"

        # å…¶ä»–æƒ…å†µï¼šæ ‡è®°ä¸ºé”™è¯¯
        return "mark_error"

    def _get_recovery_count(self, task):
        """è·å–ä»»åŠ¡å·²æ¢å¤æ¬¡æ•°

        Args:
            task: ProjectEvaluation å®ä¾‹

        Returns:
            int: æ¢å¤æ¬¡æ•°
        """
        if not task.evaluation_result:
            return 0

        try:
            result_data = json.loads(task.evaluation_result)
            recovery_count = 0

            for item in result_data:
                if isinstance(item, dict) and item.get("reason", "").startswith("æœåŠ¡é‡å¯"):
                    recovery_count += 1

            return recovery_count
        except:
            return 0

    def _auto_resume_task(self, task, error_info):
        """è‡ªåŠ¨æ¢å¤ä»»åŠ¡

        Args:
            task: ProjectEvaluation å®ä¾‹
            error_info: é”™è¯¯ä¿¡æ¯

        Returns:
            dict: æ¢å¤ç»“æœ
        """
        try:
            # é‡æ–°å¯åŠ¨ä»»åŠ¡ï¼ˆè¿™é‡Œå¯ä»¥è°ƒç”¨å®Œæ•´çš„è¯„æµ‹æµç¨‹ï¼‰
            logger.info(f"ğŸ”„ è‡ªåŠ¨æ¢å¤ä»»åŠ¡: {task.project_id}:{task.task_id}")

            # æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºå¾…å¤„ç†
            task.status = TaskStatus.PENDING.value
            task.updated_at = datetime.now()

            # è®°å½•æ¢å¤ä¿¡æ¯
            recovery_log = {
                **error_info,
                "recovery_action": "auto_resume",
                "recovered_at": datetime.now().isoformat()
            }

            if task.evaluation_result:
                existing_result = json.loads(task.evaluation_result)
                existing_result.append(recovery_log)
            else:
                existing_result = [recovery_log]

            task.evaluation_result = json.dumps(existing_result)

            logger.info(f"âœ… ä»»åŠ¡ {task.project_id}:{task.task_id} å·²æ ‡è®°ä¸ºè‡ªåŠ¨æ¢å¤")
            return {
                'action': 'recovered',
                'is_recoverable': True,
                'message': 'ä»»åŠ¡å·²æ ‡è®°ä¸ºè‡ªåŠ¨æ¢å¤'
            }

        except Exception as e:
            logger.error(f"è‡ªåŠ¨æ¢å¤ä»»åŠ¡å¤±è´¥ {task.project_id}:{task.task_id} - {e}")
            return self._mark_as_error_with_info(task, error_info)

    def _mark_for_retry(self, task, error_info):
        """æ ‡è®°ä»»åŠ¡ä¸ºå¯é‡è¯•

        Args:
            task: ProjectEvaluation å®ä¾‹
            error_info: é”™è¯¯ä¿¡æ¯

        Returns:
            dict: æ¢å¤ç»“æœ
        """
        try:
            task.status = TaskStatus.ERROR.value
            task.updated_at = datetime.now()

            # è®°å½•é‡è¯•ä¿¡æ¯
            retry_log = {
                **error_info,
                "recovery_action": "mark_for_retry",
                "retry_eligible": True,
                "marked_at": datetime.now().isoformat()
            }

            if task.evaluation_result:
                existing_result = json.loads(task.evaluation_result)
                existing_result.append(retry_log)
            else:
                existing_result = [retry_log]

            task.evaluation_result = json.dumps(existing_result)

            logger.info(f"ğŸ”„ ä»»åŠ¡ {task.project_id}:{task.task_id} å·²æ ‡è®°ä¸ºå¯é‡è¯•")
            return {
                'action': 'recovered',
                'is_recoverable': True,
                'message': 'ä»»åŠ¡å·²æ ‡è®°ä¸ºå¯é‡è¯•'
            }

        except Exception as e:
            logger.error(f"æ ‡è®°é‡è¯•å¤±è´¥ {task.project_id}:{task.task_id} - {e}")
            return {'action': 'failed', 'is_recoverable': False, 'message': str(e)}

    def _mark_as_error_with_info(self, task, error_info):
        """æ ‡è®°ä»»åŠ¡ä¸ºé”™è¯¯å¹¶ä¿ç•™è¯¦ç»†ä¿¡æ¯

        Args:
            task: ProjectEvaluation å®ä¾‹
            error_info: é”™è¯¯ä¿¡æ¯

        Returns:
            dict: æ¢å¤ç»“æœ
        """
        try:
            task.status = TaskStatus.ERROR.value
            task.updated_at = datetime.now()

            # è®°å½•è¯¦ç»†é”™è¯¯ä¿¡æ¯
            error_log = {
                **error_info,
                "recovery_action": "marked_error",
                "marked_at": datetime.now().isoformat()
            }

            if task.evaluation_result:
                existing_result = json.loads(task.evaluation_result)
                existing_result.append(error_log)
            else:
                existing_result = [error_log]

            task.evaluation_result = json.dumps(existing_result)

            logger.info(f"âŒ ä»»åŠ¡ {task.project_id}:{task.task_id} å·²æ ‡è®°ä¸ºé”™è¯¯")
            return {
                'action': 'failed',
                'is_recoverable': error_info["is_recoverable"],
                'message': 'ä»»åŠ¡å·²æ ‡è®°ä¸ºé”™è¯¯'
            }

        except Exception as e:
            logger.error(f"æ ‡è®°é”™è¯¯å¤±è´¥ {task.project_id}:{task.task_id} - {e}")
            return {'action': 'failed', 'is_recoverable': False, 'message': str(e)}

    def _mark_as_unrecoverable(self, task, error_info):
        """æ ‡è®°ä»»åŠ¡ä¸ºä¸å¯æ¢å¤

        Args:
            task: ProjectEvaluation å®ä¾‹
            error_info: é”™è¯¯ä¿¡æ¯

        Returns:
            dict: æ¢å¤ç»“æœ
        """
        try:
            task.status = TaskStatus.ERROR.value
            task.updated_at = datetime.now()

            # è®°å½•ä¸å¯æ¢å¤ä¿¡æ¯
            unrecoverable_log = {
                **error_info,
                "recovery_action": "marked_unrecoverable",
                "reason": "ä»»åŠ¡æŒç»­æ—¶é—´è¿‡é•¿æˆ–æ¢å¤æ¬¡æ•°è¶…é™",
                "marked_at": datetime.now().isoformat()
            }

            if task.evaluation_result:
                existing_result = json.loads(task.evaluation_result)
                existing_result.append(unrecoverable_log)
            else:
                existing_result = [unrecoverable_log]

            task.evaluation_result = json.dumps(existing_result)

            logger.info(f"ğŸš« ä»»åŠ¡ {task.project_id}:{task.task_id} å·²æ ‡è®°ä¸ºä¸å¯æ¢å¤")
            return {
                'action': 'ignored',
                'is_recoverable': False,
                'message': 'ä»»åŠ¡å·²æ ‡è®°ä¸ºä¸å¯æ¢å¤'
            }

        except Exception as e:
            logger.error(f"æ ‡è®°ä¸å¯æ¢å¤å¤±è´¥ {task.project_id}:{task.task_id} - {e}")
            return {'action': 'failed', 'is_recoverable': False, 'message': str(e)}


class TaskStateMachine:
    """ä»»åŠ¡çŠ¶æ€æœºç®¡ç†å™¨

    è´Ÿè´£ç®¡ç†ä»»åŠ¡çŠ¶æ€çš„è½¬æ¢è§„åˆ™å’ŒéªŒè¯ï¼š
    1. å®šä¹‰åˆæ³•çš„çŠ¶æ€è½¬æ¢è·¯å¾„
    2. éªŒè¯çŠ¶æ€è½¬æ¢çš„åˆæ³•æ€§
    3. æä¾›çŠ¶æ€å˜æ›´çš„äº‹åŠ¡å¤„ç†
    4. è®°å½•çŠ¶æ€å˜æ›´å†å²
    """

    # å®šä¹‰åˆæ³•çš„çŠ¶æ€è½¬æ¢è§„åˆ™
    VALID_TRANSITIONS = {
        TaskStatus.IDLE.value: [TaskStatus.PENDING.value],
        TaskStatus.PENDING.value: [TaskStatus.SYNCING.value, TaskStatus.CANCELLED.value, TaskStatus.ERROR.value],
        TaskStatus.SYNCING.value: [TaskStatus.EVALUATING.value, TaskStatus.ERROR.value, TaskStatus.CANCELLED.value],
        TaskStatus.EVALUATING.value: [TaskStatus.COMPLETED.value, TaskStatus.ERROR.value, TaskStatus.PAUSED.value],
        TaskStatus.PAUSED.value: [TaskStatus.EVALUATING.value, TaskStatus.CANCELLED.value],
        TaskStatus.COMPLETED.value: [TaskStatus.PENDING.value],  # å¯ä»¥é‡æ–°å¼€å§‹è¯„æµ‹
        TaskStatus.ERROR.value: [TaskStatus.PENDING.value],      # å¯ä»¥é‡è¯•
        TaskStatus.CANCELLED.value: [TaskStatus.PENDING.value]   # å¯ä»¥é‡æ–°å¼€å§‹
    }

    # ç»ˆç«¯çŠ¶æ€ï¼ˆæ— æ³•å†è½¬æ¢åˆ°å…¶ä»–çŠ¶æ€ï¼Œé™¤éç‰¹æ®Šå¤„ç†ï¼‰
    TERMINAL_STATES = [TaskStatus.COMPLETED.value, TaskStatus.CANCELLED.value]

    @classmethod
    def is_valid_transition(cls, current_status, new_status):
        """æ£€æŸ¥çŠ¶æ€è½¬æ¢æ˜¯å¦åˆæ³•

        Args:
            current_status: å½“å‰çŠ¶æ€
            new_status: ç›®æ ‡çŠ¶æ€

        Returns:
            bool: è½¬æ¢æ˜¯å¦åˆæ³•
        """
        if current_status not in cls.VALID_TRANSITIONS:
            logger.warning(f"æœªçŸ¥çŠ¶æ€: {current_status}")
            return False

        return new_status in cls.VALID_TRANSITIONS[current_status]

    @classmethod
    def get_valid_next_states(cls, current_status):
        """è·å–å½“å‰çŠ¶æ€å¯ä»¥è½¬æ¢åˆ°çš„æ‰€æœ‰åˆæ³•çŠ¶æ€

        Args:
            current_status: å½“å‰çŠ¶æ€

        Returns:
            list: å¯è½¬æ¢çš„çŠ¶æ€åˆ—è¡¨
        """
        return cls.VALID_TRANSITIONS.get(current_status, [])

    @classmethod
    def is_terminal_state(cls, status):
        """æ£€æŸ¥æ˜¯å¦ä¸ºç»ˆç«¯çŠ¶æ€

        Args:
            status: çŠ¶æ€å€¼

        Returns:
            bool: æ˜¯å¦ä¸ºç»ˆç«¯çŠ¶æ€
        """
        return status in cls.TERMINAL_STATES

    @classmethod
    def validate_state_transition(cls, project_id, task_id, current_status, new_status, reason=None):
        """éªŒè¯çŠ¶æ€è½¬æ¢å¹¶è¿”å›è¯¦ç»†ä¿¡æ¯

        Args:
            project_id: é¡¹ç›®ID
            task_id: ä»»åŠ¡ID
            current_status: å½“å‰çŠ¶æ€
            new_status: ç›®æ ‡çŠ¶æ€
            reason: è½¬æ¢åŸå› 

        Returns:
            dict: éªŒè¯ç»“æœ
        """
        result = {
            'valid': False,
            'current_status': current_status,
            'new_status': new_status,
            'reason': reason or 'çŠ¶æ€å˜æ›´',
            'error_message': None
        }

        # æ£€æŸ¥å½“å‰çŠ¶æ€æ˜¯å¦æœ‰æ•ˆ
        if current_status not in cls.VALID_TRANSITIONS:
            result['error_message'] = f'æ— æ•ˆçš„å½“å‰çŠ¶æ€: {current_status}'
            return result

        # æ£€æŸ¥æ–°çŠ¶æ€æ˜¯å¦æœ‰æ•ˆ
        if new_status not in cls.VALID_TRANSITIONS:
            result['error_message'] = f'æ— æ•ˆçš„ç›®æ ‡çŠ¶æ€: {new_status}'
            return result

        # æ£€æŸ¥è½¬æ¢æ˜¯å¦åˆæ³•
        if not cls.is_valid_transition(current_status, new_status):
            valid_states = cls.get_valid_next_states(current_status)
            result['error_message'] = (
                f'çŠ¶æ€è½¬æ¢ {current_status} -> {new_status} ä¸åˆæ³•ã€‚'
                f'å½“å‰çŠ¶æ€å¯è½¬æ¢åˆ°: {", ".join(valid_states)}'
            )
            return result

        result['valid'] = True
        return result

    @classmethod
    def update_task_status_safely(cls, project_id, task_id, new_status, error_msg=None, session=None):
        """å®‰å…¨æ›´æ–°ä»»åŠ¡çŠ¶æ€ï¼ˆå¸¦äº‹åŠ¡å¤„ç†ï¼‰

        Args:
            project_id: é¡¹ç›®ID
            task_id: ä»»åŠ¡ID
            new_status: æ–°çŠ¶æ€
            error_msg: é”™è¯¯ä¿¡æ¯ï¼ˆå¦‚æœçŠ¶æ€ä¸ºERRORï¼‰
            session: æ•°æ®åº“ä¼šè¯ï¼ˆå¯é€‰ï¼‰

        Returns:
            dict: æ›´æ–°ç»“æœ
        """
        result = {
            'success': False,
            'old_status': None,
            'new_status': new_status,
            'message': None,
            'error': None
        }

        use_external_session = session is not None
        if not use_external_session:
            session = db.session

        try:
            # æŸ¥æ‰¾ä»»åŠ¡è®°å½•
            evaluation = session.query(ProjectEvaluation).filter_by(
                project_id=project_id,
                task_id=task_id
            ).first()

            if not evaluation:
                result['error'] = f'ä»»åŠ¡ä¸å­˜åœ¨: {project_id}:{task_id}'
                return result

            old_status = evaluation.status
            result['old_status'] = old_status

            # éªŒè¯çŠ¶æ€è½¬æ¢
            validation = cls.validate_state_transition(
                project_id, task_id, old_status, new_status
            )

            if not validation['valid']:
                result['error'] = f'çŠ¶æ€è½¬æ¢éªŒè¯å¤±è´¥: {validation["error_message"]}'
                return result

            # è®°å½•çŠ¶æ€å˜æ›´å†å²
            state_change_log = {
                'timestamp': datetime.now().isoformat(),
                'old_status': old_status,
                'new_status': new_status,
                'reason': validation['reason'],
                'validation_passed': True
            }

            # æ›´æ–°ä»»åŠ¡çŠ¶æ€
            evaluation.status = new_status
            evaluation.updated_at = datetime.now()

            # å¦‚æœæ˜¯é”™è¯¯çŠ¶æ€ï¼Œè®°å½•é”™è¯¯ä¿¡æ¯
            if new_status == TaskStatus.ERROR.value and error_msg:
                error_record = {
                    'timestamp': datetime.now().isoformat(),
                    'error_message': error_msg,
                    'state_change_log': state_change_log
                }

                if evaluation.evaluation_result:
                    existing_result = json.loads(evaluation.evaluation_result)
                    if isinstance(existing_result, list):
                        existing_result.append(error_record)
                    else:
                        existing_result = [existing_result, error_record]
                else:
                    existing_result = [error_record]

                evaluation.evaluation_result = json.dumps(existing_result)

            # æäº¤äº‹åŠ¡ï¼ˆå¦‚æœä½¿ç”¨å¤–éƒ¨ä¼šè¯ï¼Œç”±å¤–éƒ¨ç®¡ç†æäº¤ï¼‰
            if not use_external_session:
                session.commit()

            logger.info(f"âœ… çŠ¶æ€æ›´æ–°æˆåŠŸ: {project_id}:{task_id} {old_status} -> {new_status}")
            result['success'] = True
            result['message'] = f'çŠ¶æ€å·²ä» {old_status} æ›´æ–°ä¸º {new_status}'

            return result

        except Exception as e:
            if not use_external_session:
                session.rollback()

            logger.error(f"çŠ¶æ€æ›´æ–°å¤±è´¥ {project_id}:{task_id} - {e}")
            result['error'] = f'æ•°æ®åº“æ“ä½œå¤±è´¥: {str(e)}'
            return result

    @classmethod
    def create_task(cls, project_id, task_id, initial_status=None):
        """åˆ›å»ºæ–°ä»»åŠ¡å¹¶è®¾ç½®åˆå§‹çŠ¶æ€

        Args:
            project_id: é¡¹ç›®ID
            task_id: ä»»åŠ¡ID
            initial_status: åˆå§‹çŠ¶æ€ï¼ˆé»˜è®¤ä¸ºPENDINGï¼‰

        Returns:
            dict: åˆ›å»ºç»“æœ
        """
        if initial_status is None:
            initial_status = TaskStatus.PENDING.value

        # éªŒè¯åˆå§‹çŠ¶æ€
        if initial_status not in [TaskStatus.IDLE.value, TaskStatus.PENDING.value]:
            return {
                'success': False,
                'error': f'æ— æ•ˆçš„åˆå§‹çŠ¶æ€: {initial_status}ã€‚å¿…é¡»æ˜¯ IDLE æˆ– PENDING'
            }

        try:
            # æ£€æŸ¥ä»»åŠ¡æ˜¯å¦å·²å­˜åœ¨
            existing_task = ProjectEvaluation.query.filter_by(
                project_id=project_id,
                task_id=task_id
            ).first()

            if existing_task:
                return {
                    'success': False,
                    'error': f'ä»»åŠ¡å·²å­˜åœ¨: {project_id}:{task_id}',
                    'existing_status': existing_task.status
                }

            # åˆ›å»ºæ–°ä»»åŠ¡
            new_task = ProjectEvaluation(
                project_id=project_id,
                task_id=task_id,
                status=initial_status,
                created_at=datetime.now(),
                updated_at=datetime.now()
            )

            db.session.add(new_task)
            db.session.commit()

            logger.info(f"âœ… ä»»åŠ¡åˆ›å»ºæˆåŠŸ: {project_id}:{task_id} (çŠ¶æ€: {initial_status})")

            return {
                'success': True,
                'task_id': task_id,
                'project_id': project_id,
                'status': initial_status,
                'message': f'ä»»åŠ¡åˆ›å»ºæˆåŠŸï¼Œåˆå§‹çŠ¶æ€: {initial_status}'
            }

        except Exception as e:
            db.session.rollback()
            logger.error(f"ä»»åŠ¡åˆ›å»ºå¤±è´¥ {project_id}:{task_id} - {e}")

            return {
                'success': False,
                'error': f'ä»»åŠ¡åˆ›å»ºå¤±è´¥: {str(e)}'
            }

    @classmethod
    def get_task_state_history(cls, project_id, task_id):
        """è·å–ä»»åŠ¡çŠ¶æ€å˜æ›´å†å²

        Args:
            project_id: é¡¹ç›®ID
            task_id: ä»»åŠ¡ID

        Returns:
            dict: çŠ¶æ€å†å²
        """
        try:
            evaluation = ProjectEvaluation.query.filter_by(
                project_id=project_id,
                task_id=task_id
            ).first()

            if not evaluation:
                return {
                    'success': False,
                    'error': f'ä»»åŠ¡ä¸å­˜åœ¨: {project_id}:{task_id}'
                }

            # è§£æçŠ¶æ€å˜æ›´å†å²
            history = []
            if evaluation.evaluation_result:
                try:
                    result_data = json.loads(evaluation.evaluation_result)
                    for entry in result_data:
                        if isinstance(entry, dict) and 'state_change_log' in entry:
                            history.append(entry['state_change_log'])
                        elif isinstance(entry, dict) and 'timestamp' in entry and 'reason' in entry:
                            # ç®€å•çš„çŠ¶æ€å˜æ›´è®°å½•
                            history.append(entry)
                except:
                    logger.warning(f"è§£æçŠ¶æ€å†å²å¤±è´¥: {project_id}:{task_id}")

            # æ·»åŠ å½“å‰çŠ¶æ€ä¿¡æ¯
            current_info = {
                'timestamp': datetime.now().isoformat(),
                'current_status': evaluation.status,
                'last_updated': evaluation.updated_at.isoformat() if evaluation.updated_at else None,
                'created_at': evaluation.created_at.isoformat() if evaluation.created_at else None
            }

            return {
                'success': True,
                'project_id': project_id,
                'task_id': task_id,
                'current_status': evaluation.status,
                'state_history': history,
                'current_info': current_info
            }

        except Exception as e:
            logger.error(f"è·å–çŠ¶æ€å†å²å¤±è´¥ {project_id}:{task_id} - {e}")
            return {
                'success': False,
                'error': f'è·å–çŠ¶æ€å†å²å¤±è´¥: {str(e)}'
            }


class TaskMonitor:
    """ä»»åŠ¡ç›‘æ§å™¨

    è´Ÿè´£ç›‘æ§æ´»è·ƒä»»åŠ¡å¹¶å¤„ç†è¶…æ—¶æƒ…å†µï¼š
    1. å®æ—¶ç›‘æ§ä»»åŠ¡æ‰§è¡ŒçŠ¶æ€
    2. æ£€æµ‹å’Œå¤„ç†è¶…æ—¶ä»»åŠ¡
    3. æä¾›ä»»åŠ¡å¥åº·æ£€æŸ¥
    4. è‡ªåŠ¨å¤„ç†å¼‚å¸¸æƒ…å†µ
    """

    def __init__(self):
        self.default_timeout_minutes = 30  # é»˜è®¤è¶…æ—¶æ—¶é—´ï¼š30åˆ†é’Ÿ
        self.check_interval_seconds = 60   # æ£€æŸ¥é—´éš”ï¼š60ç§’
        self.max_consecutive_failures = 3  # æœ€å¤§è¿ç»­å¤±è´¥æ¬¡æ•°
        self.monitoring_enabled = True
        self.monitoring_thread = None
        self.stop_event = threading.Event()

        # ä»»åŠ¡çŠ¶æ€è¶…æ—¶é…ç½®ï¼ˆåˆ†é’Ÿï¼‰
        self.state_timeouts = {
            TaskStatus.PENDING.value: 5,      # å¾…å¤„ç†çŠ¶æ€ï¼š5åˆ†é’Ÿ
            TaskStatus.SYNCING.value: 15,     # åŒæ­¥çŠ¶æ€ï¼š15åˆ†é’Ÿ
            TaskStatus.EVALUATING.value: 30,  # è¯„æµ‹çŠ¶æ€ï¼š30åˆ†é’Ÿ
            TaskStatus.PAUSED.value: 120      # æš‚åœçŠ¶æ€ï¼š120åˆ†é’Ÿ
        }

    def start_monitoring(self):
        """å¯åŠ¨ä»»åŠ¡ç›‘æ§"""
        if not self.monitoring_enabled:
            logger.info("âš ï¸ ä»»åŠ¡ç›‘æ§å·²ç¦ç”¨")
            return

        if self.monitoring_thread and self.monitoring_thread.is_alive():
            logger.info("ğŸ“Š ä»»åŠ¡ç›‘æ§å·²åœ¨è¿è¡Œä¸­")
            return

        self.stop_event.clear()
        self.monitoring_thread = threading.Thread(
            target=self._monitoring_loop,
            name="TaskMonitor",
            daemon=True
        )
        self.monitoring_thread.start()
        logger.info(f"ğŸ“Š ä»»åŠ¡ç›‘æ§å·²å¯åŠ¨ï¼Œæ£€æŸ¥é—´éš”: {self.check_interval_seconds}ç§’")

    def stop_monitoring(self):
        """åœæ­¢ä»»åŠ¡ç›‘æ§"""
        if self.monitoring_thread and self.monitoring_thread.is_alive():
            logger.info("ğŸ›‘ æ­£åœ¨åœæ­¢ä»»åŠ¡ç›‘æ§...")
            self.stop_event.set()
            self.monitoring_thread.join(timeout=10)
            logger.info("âœ… ä»»åŠ¡ç›‘æ§å·²åœæ­¢")

    def _monitoring_loop(self):
        """ç›‘æ§ä¸»å¾ªç¯"""
        consecutive_failures = 0

        while not self.stop_event.is_set():
            try:
                # æ‰§è¡Œå¥åº·æ£€æŸ¥
                health_result = self.perform_health_check()

                if health_result['issues_found'] > 0:
                    logger.warning(f"ğŸ” å‘ç° {health_result['issues_found']} ä¸ªä»»åŠ¡é—®é¢˜")

                # æ£€æŸ¥è¶…æ—¶ä»»åŠ¡
                timeout_result = self.check_timeout_tasks()

                if timeout_result['timeout_tasks'] > 0:
                    logger.warning(f"â° å‘ç° {timeout_result['timeout_tasks']} ä¸ªè¶…æ—¶ä»»åŠ¡")

                # é‡ç½®å¤±è´¥è®¡æ•°
                consecutive_failures = 0

            except Exception as e:
                consecutive_failures += 1
                logger.error(f"âŒ ä»»åŠ¡ç›‘æ§æ£€æŸ¥å¤±è´¥ ({consecutive_failures}/{self.max_consecutive_failures}): {e}")

                # è¿ç»­å¤±è´¥è¿‡å¤šæ—¶åœæ­¢ç›‘æ§
                if consecutive_failures >= self.max_consecutive_failures:
                    logger.error(f"ğŸš¨ ä»»åŠ¡ç›‘æ§è¿ç»­å¤±è´¥ {consecutive_failures} æ¬¡ï¼Œåœæ­¢ç›‘æ§")
                    break

            # ç­‰å¾…ä¸‹æ¬¡æ£€æŸ¥
            self.stop_event.wait(self.check_interval_seconds)

    def perform_health_check(self):
        """æ‰§è¡Œä»»åŠ¡å¥åº·æ£€æŸ¥

        Returns:
            dict: å¥åº·æ£€æŸ¥ç»“æœ
        """
        health_result = {
            'total_tasks': 0,
            'active_tasks': 0,
            'issues_found': 0,
            'issues': []
        }

        try:
            with app.app_context():
                # æŸ¥è¯¢æ‰€æœ‰æ´»è·ƒä»»åŠ¡
                active_tasks = ProjectEvaluation.query.filter(
                    ProjectEvaluation.status.in_([
                        TaskStatus.PENDING.value,
                        TaskStatus.SYNCING.value,
                        TaskStatus.EVALUATING.value,
                        TaskStatus.PAUSED.value
                    ])
                ).all()

                health_result['total_tasks'] = len(active_tasks)
                health_result['active_tasks'] = len(active_tasks)

                current_time = datetime.now()

                for task in active_tasks:
                    task_age = current_time - task.created_at
                    timeout_limit = self.state_timeouts.get(
                        task.status,
                        self.default_timeout_minutes
                    )
                    timeout_threshold = timedelta(minutes=timeout_limit)

                    issues = []

                    # æ£€æŸ¥æ›´æ–°æ—¶é—´ï¼ˆä¸»è¦ä¾æ®ï¼‰
                    update_age = current_time - task.updated_at if task.updated_at else task_age
                    if update_age > timeout_threshold:
                        issues.append(f"ä»»åŠ¡åœ¨çŠ¶æ€åœç•™è¿‡ä¹…ï¼ˆ{update_age.total_seconds()/60:.1f}åˆ†é’Ÿï¼‰")

                    # æ£€æŸ¥ä»»åŠ¡çŠ¶æ€ä¸€è‡´æ€§ï¼ˆä½¿ç”¨update_ageï¼‰
                    if task.status == TaskStatus.PENDING.value and update_age > timedelta(minutes=10):
                        issues.append("ä»»åŠ¡é•¿æ—¶é—´å¤„äºå¾…å¤„ç†çŠ¶æ€")

                    # å¦‚æœå‘ç°é—®é¢˜ï¼Œè®°å½•åˆ°ç»“æœä¸­
                    if issues:
                        health_result['issues_found'] += 1
                        health_result['issues'].append({
                            'project_id': task.project_id,
                            'task_id': task.task_id,
                            'status': task.status,
                            'age_minutes': task_age.total_seconds() / 60,
                            'created_at': task.created_at.isoformat(),
                            'updated_at': task.updated_at.isoformat() if task.updated_at else None,
                            'issues': issues
                        })

        except Exception as e:
            logger.error(f"å¥åº·æ£€æŸ¥æ‰§è¡Œå¤±è´¥: {e}")
            health_result['error'] = str(e)

        return health_result

    def check_timeout_tasks(self):
        """æ£€æŸ¥å¹¶å¤„ç†è¶…æ—¶ä»»åŠ¡

        Returns:
            dict: è¶…æ—¶æ£€æŸ¥ç»“æœ
        """
        timeout_result = {
            'timeout_tasks': 0,
            'processed_tasks': 0,
            'failed_tasks': 0,
            'details': []
        }

        try:
            with app.app_context():
                current_time = datetime.now()
                timeout_threshold = timedelta(minutes=self.default_timeout_minutes)

                # æŸ¥è¯¢å¯èƒ½çš„è¶…æ—¶ä»»åŠ¡
                potential_timeout_tasks = ProjectEvaluation.query.filter(
                    ProjectEvaluation.status.in_([
                        TaskStatus.PENDING.value,
                        TaskStatus.SYNCING.value,
                        TaskStatus.EVALUATING.value,
                        TaskStatus.PAUSED.value
                    ])
                ).all()

                for task in potential_timeout_tasks:
                    task_result = self._check_single_task_timeout(task, current_time)
                    timeout_result['details'].append(task_result)

                    if task_result['is_timeout']:
                        timeout_result['timeout_tasks'] += 1

                        # å°è¯•å¤„ç†è¶…æ—¶ä»»åŠ¡
                        if self._handle_timeout_task(task, task_result):
                            timeout_result['processed_tasks'] += 1
                        else:
                            timeout_result['failed_tasks'] += 1

        except Exception as e:
            logger.error(f"è¶…æ—¶æ£€æŸ¥æ‰§è¡Œå¤±è´¥: {e}")
            timeout_result['error'] = str(e)

        return timeout_result

    def _check_single_task_timeout(self, task, current_time):
        """æ£€æŸ¥å•ä¸ªä»»åŠ¡æ˜¯å¦è¶…æ—¶

        Args:
            task: ProjectEvaluation å®ä¾‹
            current_time: å½“å‰æ—¶é—´

        Returns:
            dict: è¶…æ—¶æ£€æŸ¥ç»“æœ
        """
        task_age = current_time - task.created_at
        update_age = current_time - task.updated_at if task.updated_at else task_age

        # è·å–çŠ¶æ€ç‰¹å®šçš„è¶…æ—¶æ—¶é—´
        state_timeout = self.state_timeouts.get(
            task.status,
            self.default_timeout_minutes
        )
        state_timeout_threshold = timedelta(minutes=state_timeout)

        # é»˜è®¤è¶…æ—¶æ£€æŸ¥
        default_timeout_threshold = timedelta(minutes=self.default_timeout_minutes)

        result = {
            'project_id': task.project_id,
            'task_id': task.task_id,
            'status': task.status,
            'task_age_minutes': task_age.total_seconds() / 60,
            'update_age_minutes': update_age.total_seconds() / 60,
            'state_timeout_minutes': state_timeout,
            'is_timeout': False,
            'timeout_type': None,
            'timeout_reason': None
        }

        # æ£€æŸ¥çŠ¶æ€è¶…æ—¶ï¼ˆä½¿ç”¨update_ageè€Œä¸æ˜¯task_ageï¼‰
        if update_age > state_timeout_threshold:
            result['is_timeout'] = True
            result['timeout_type'] = 'state_timeout'
            result['timeout_reason'] = f"ä»»åŠ¡åœ¨ {task.status} çŠ¶æ€è¶…è¿‡ {state_timeout} åˆ†é’Ÿ"

        # æ£€æŸ¥é»˜è®¤è¶…æ—¶ï¼ˆä½¿ç”¨update_ageè€Œä¸æ˜¯task_ageï¼‰
        elif update_age > default_timeout_threshold:
            result['is_timeout'] = True
            result['timeout_type'] = 'default_timeout'
            result['timeout_reason'] = f"ä»»åŠ¡æ‰§è¡Œè¶…è¿‡ {self.default_timeout_minutes} åˆ†é’Ÿ"

        # æ£€æŸ¥é•¿æ—¶é—´æœªæ›´æ–°
        elif update_age > state_timeout_threshold * 2:  # æ›´æ–°æ—¶é—´çš„é˜ˆå€¼æ›´å®½æ¾
            result['is_timeout'] = True
            result['timeout_type'] = 'update_timeout'
            result['timeout_reason'] = f"ä»»åŠ¡è¶…è¿‡ {state_timeout * 2} åˆ†é’Ÿæœªæ›´æ–°"

        return result

    def _handle_timeout_task(self, task, timeout_info):
        """å¤„ç†è¶…æ—¶ä»»åŠ¡

        Args:
            task: ProjectEvaluation å®ä¾‹
            timeout_info: è¶…æ—¶ä¿¡æ¯

        Returns:
            bool: å¤„ç†æ˜¯å¦æˆåŠŸ
        """
        try:
            logger.warning(f"â° å¤„ç†è¶…æ—¶ä»»åŠ¡: {task.project_id}:{task.task_id} "
                         f"({timeout_info['timeout_reason']})")

            # æ ¹æ®ä»»åŠ¡çŠ¶æ€å’Œè¶…æ—¶ç±»å‹é‡‡å–ä¸åŒå¤„ç†ç­–ç•¥
            if task.status == TaskStatus.PENDING.value:
                # å¾…å¤„ç†ä»»åŠ¡è¶…æ—¶ï¼šæ ‡è®°ä¸ºé”™è¯¯ï¼Œå…è®¸é‡è¯•
                state_machine_result = TaskStateMachine.update_task_status_safely(
                    task.project_id,
                    task.task_id,
                    TaskStatus.ERROR.value,
                    f"ä»»åŠ¡å¾…å¤„ç†è¶…æ—¶: {timeout_info['timeout_reason']}"
                )

                return state_machine_result['success']

            elif task.status in [TaskStatus.SYNCING.value, TaskStatus.EVALUATING.value]:
                # æ‰§è¡Œä¸­ä»»åŠ¡è¶…æ—¶ï¼šæ ¹æ®è¶…æ—¶æ—¶é—´å†³å®šå¤„ç†æ–¹å¼
                task_age = timeout_info['task_age_minutes']

                if task_age > self.default_timeout_minutes * 2:
                    # è¶…æ—¶æ—¶é—´è¿‡é•¿ï¼šæ ‡è®°ä¸ºé”™è¯¯ä½†ä¸å…è®¸è‡ªåŠ¨é‡è¯•
                    timeout_record = {
                        'timestamp': datetime.now().isoformat(),
                        'timeout_type': timeout_info['timeout_type'],
                        'timeout_reason': timeout_info['timeout_reason'],
                        'task_age_minutes': task_age,
                        'action_taken': 'marked_as_error_no_retry'
                    }

                    # æ›´æ–°ä»»åŠ¡çŠ¶æ€
                    task.status = TaskStatus.ERROR.value
                    task.updated_at = datetime.now()

                    # è®°å½•è¶…æ—¶ä¿¡æ¯
                    if task.evaluation_result:
                        existing_result = json.loads(task.evaluation_result)
                        existing_result.append(timeout_record)
                    else:
                        existing_result = [timeout_record]

                    task.evaluation_result = json.dumps(existing_result)
                    db.session.commit()

                    logger.info(f"âŒ ä»»åŠ¡ {task.project_id}:{task.task_id} è¶…æ—¶è¿‡é•¿ï¼Œæ ‡è®°ä¸ºé”™è¯¯ï¼ˆä¸å…è®¸é‡è¯•ï¼‰")
                    return True
                else:
                    # è¶…æ—¶æ—¶é—´è¾ƒçŸ­ï¼šæ ‡è®°ä¸ºé”™è¯¯ï¼Œå…è®¸é‡è¯•
                    state_machine_result = TaskStateMachine.update_task_status_safely(
                        task.project_id,
                        task.task_id,
                        TaskStatus.ERROR.value,
                        f"ä»»åŠ¡æ‰§è¡Œè¶…æ—¶: {timeout_info['timeout_reason']}"
                    )

                    return state_machine_result['success']

            elif task.status == TaskStatus.PAUSED.value:
                # æš‚åœä»»åŠ¡è¶…æ—¶ï¼šè‡ªåŠ¨å–æ¶ˆ
                state_machine_result = TaskStateMachine.update_task_status_safely(
                    task.project_id,
                    task.task_id,
                    TaskStatus.CANCELLED.value,
                    f"æš‚åœä»»åŠ¡è¶…æ—¶è‡ªåŠ¨å–æ¶ˆ: {timeout_info['timeout_reason']}"
                )

                return state_machine_result['success']

            else:
                logger.warning(f"æœªçŸ¥ä»»åŠ¡çŠ¶æ€è¶…æ—¶: {task.project_id}:{task.task_id} ({task.status})")
                return False

        except Exception as e:
            logger.error(f"å¤„ç†è¶…æ—¶ä»»åŠ¡å¤±è´¥ {task.project_id}:{task.task_id} - {e}")
            return False

    def get_monitoring_status(self):
        """è·å–ç›‘æ§çŠ¶æ€ä¿¡æ¯

        Returns:
            dict: ç›‘æ§çŠ¶æ€
        """
        return {
            'monitoring_enabled': self.monitoring_enabled,
            'is_running': self.monitoring_thread and self.monitoring_thread.is_alive(),
            'check_interval_seconds': self.check_interval_seconds,
            'default_timeout_minutes': self.default_timeout_minutes,
            'state_timeouts': self.state_timeouts,
            'start_time': getattr(self, 'start_time', None)
        }

    def update_config(self, **kwargs):
        """æ›´æ–°ç›‘æ§é…ç½®

        Args:
            **kwargs: é…ç½®å‚æ•°
        """
        if 'check_interval_seconds' in kwargs:
            self.check_interval_seconds = max(10, kwargs['check_interval_seconds'])

        if 'default_timeout_minutes' in kwargs:
            self.default_timeout_minutes = max(5, kwargs['default_timeout_minutes'])

        if 'state_timeouts' in kwargs:
            self.state_timeouts.update(kwargs['state_timeouts'])

        if 'monitoring_enabled' in kwargs:
            was_enabled = self.monitoring_enabled
            self.monitoring_enabled = kwargs['monitoring_enabled']

            # çŠ¶æ€æ”¹å˜æ—¶å¯åŠ¨æˆ–åœæ­¢ç›‘æ§
            if not was_enabled and self.monitoring_enabled:
                self.start_monitoring()
            elif was_enabled and not self.monitoring_enabled:
                self.stop_monitoring()

        logger.info(f"ğŸ“Š ä»»åŠ¡ç›‘æ§é…ç½®å·²æ›´æ–°: {kwargs}")


class ThreadSafeManager:
    """çº¿ç¨‹å®‰å…¨ç®¡ç†å™¨

    æä¾›çº¿ç¨‹å®‰å…¨çš„æ•°æ®åº“æ“ä½œå’Œèµ„æºç®¡ç†ï¼š
    1. çº¿ç¨‹æœ¬åœ°å­˜å‚¨ç®¡ç†
    2. æ•°æ®åº“è¿æ¥æ± ç®¡ç†
    3. çº¿ç¨‹é”å’ŒåŒæ­¥æœºåˆ¶
    4. èµ„æºç”Ÿå‘½å‘¨æœŸç®¡ç†
    """

    def __init__(self):
        self._local = threading.local()
        self._locks = {}
        self._connection_pool_size = 20
        self._session_timeout = 300  # 5åˆ†é’Ÿè¶…æ—¶

    @contextmanager
    def get_session(self, with_lock=False, lock_key=None):
        """è·å–çº¿ç¨‹å®‰å…¨çš„æ•°æ®åº“ä¼šè¯

        Args:
            with_lock: æ˜¯å¦ä½¿ç”¨é”
            lock_key: é”çš„é”®å

        Yields:
            Session: æ•°æ®åº“ä¼šè¯å¯¹è±¡
        """
        session = None
        lock = None

        try:
            # è·å–æˆ–åˆ›å»ºçº¿ç¨‹æœ¬åœ°çš„ä¼šè¯
            if not hasattr(self._local, 'session') or not self._local.session:
                self._local.session = db.session()

            session = self._local.session

            # å¦‚æœéœ€è¦é”
            if with_lock:
                lock_key = lock_key or 'default_db_lock'
                lock = self._get_lock(lock_key)
                lock.acquire()

            yield session

        except Exception as e:
            if session:
                try:
                    session.rollback()
                except:
                    pass
            logger.error(f"æ•°æ®åº“æ“ä½œå¼‚å¸¸: {e}")
            raise
        finally:
            if lock:
                lock.release()

    def _get_lock(self, key):
        """è·å–æˆ–åˆ›å»ºé”å¯¹è±¡"""
        if key not in self._locks:
            self._locks[key] = threading.RLock()
        return self._locks[key]

    @contextmanager
    def with_lock(self, key, timeout=30):
        """ä½¿ç”¨æŒ‡å®šé”çš„ä¸Šä¸‹æ–‡ç®¡ç†å™¨

        Args:
            key: é”çš„é”®å
            timeout: è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰

        Yields:
            None
        """
        lock = self._get_lock(key)
        acquired = lock.acquire(timeout=timeout)

        if not acquired:
            raise TimeoutError(f"è·å–é” {key} è¶…æ—¶")

        try:
            yield
        finally:
            lock.release()

    def cleanup_thread_resources(self):
        """æ¸…ç†çº¿ç¨‹èµ„æº"""
        if hasattr(self._local, 'session'):
            try:
                self._local.session.remove()
                del self._local.session
            except:
                pass

    def get_thread_id(self):
        """è·å–å½“å‰çº¿ç¨‹ID"""
        return threading.current_thread().ident


class TransactionManager:
    """äº‹åŠ¡ç®¡ç†å™¨

    æä¾›å¢å¼ºçš„äº‹åŠ¡ç®¡ç†åŠŸèƒ½ï¼š
    1. è‡ªåŠ¨é‡è¯•æœºåˆ¶
    2. åµŒå¥—äº‹åŠ¡æ”¯æŒ
    3. äº‹åŠ¡éš”ç¦»çº§åˆ«æ§åˆ¶
    4. å¼‚å¸¸å¤„ç†å’Œå›æ»š
    """

    def __init__(self, max_retries=3, retry_delay=1.0):
        self.max_retries = max_retries
        self.retry_delay = retry_delay
        self.thread_manager = ThreadSafeManager()

    @contextmanager
    def transaction(self, isolation_level=None, auto_retry=True, retry_on=None):
        """äº‹åŠ¡ä¸Šä¸‹æ–‡ç®¡ç†å™¨

        Args:
            isolation_level: äº‹åŠ¡éš”ç¦»çº§åˆ«
            auto_retry: æ˜¯å¦è‡ªåŠ¨é‡è¯•
            retry_on: éœ€è¦é‡è¯•çš„å¼‚å¸¸ç±»å‹ï¼ˆå…ƒç»„ï¼‰

        Yields:
            Session: æ•°æ®åº“ä¼šè¯å¯¹è±¡
        """
        if retry_on is None:
            retry_on = (Exception,)

        session = None
        retry_count = 0
        last_exception = None

        while retry_count <= self.max_retries:
            try:
                with self.thread_manager.get_session() as session:
                    # è®¾ç½®éš”ç¦»çº§åˆ«
                    if isolation_level:
                        if hasattr(session, 'execute'):
                            session.execute(text(f"SET TRANSACTION ISOLATION LEVEL {isolation_level}"))

                    # å¼€å§‹äº‹åŠ¡
                    session.begin_nested()

                    yield session

                    # æäº¤äº‹åŠ¡
                    session.commit()
                    break

            except Exception as e:
                last_exception = e

                # å›æ»šäº‹åŠ¡
                if session:
                    try:
                        session.rollback()
                    except:
                        pass

                retry_count += 1

                # æ£€æŸ¥æ˜¯å¦éœ€è¦é‡è¯•
                if auto_retry and retry_count <= self.max_retries and any(isinstance(e, exc) for exc in retry_on):
                    logger.warning(f"äº‹åŠ¡å¤±è´¥ï¼Œå‡†å¤‡ç¬¬ {retry_count} æ¬¡é‡è¯•: {e}")
                    time.sleep(self.retry_delay * (2 ** (retry_count - 1)))  # æŒ‡æ•°é€€é¿
                    continue
                else:
                    logger.error(f"äº‹åŠ¡æœ€ç»ˆå¤±è´¥: {e}")
                    break

        if last_exception and retry_count > self.max_retries:
            raise last_exception

    def execute_with_retry(self, func, *args, **kwargs):
        """å¸¦é‡è¯•æœºåˆ¶æ‰§è¡Œå‡½æ•°

        Args:
            func: è¦æ‰§è¡Œçš„å‡½æ•°
            *args: å‡½æ•°å‚æ•°
            **kwargs: å‡½æ•°å…³é”®å­—å‚æ•°

        Returns:
            å‡½æ•°æ‰§è¡Œç»“æœ
        """
        retry_count = 0
        last_exception = None

        while retry_count <= self.max_retries:
            try:
                with self.transaction(auto_retry=False):
                    return func(*args, **kwargs)

            except Exception as e:
                last_exception = e
                retry_count += 1

                if retry_count <= self.max_retries:
                    logger.warning(f"å‡½æ•°æ‰§è¡Œå¤±è´¥ï¼Œå‡†å¤‡ç¬¬ {retry_count} æ¬¡é‡è¯•: {e}")
                    time.sleep(self.retry_delay * (2 ** (retry_count - 1)))
                    continue
                else:
                    logger.error(f"å‡½æ•°æ‰§è¡Œæœ€ç»ˆå¤±è´¥: {e}")
                    break

        if last_exception:
            raise last_exception

    def atomic_update(self, model_class, filter_conditions, update_data, **kwargs):
        """åŸå­æ€§æ›´æ–°æ“ä½œ

        Args:
            model_class: æ¨¡å‹ç±»
            filter_conditions: è¿‡æ»¤æ¡ä»¶
            update_data: æ›´æ–°æ•°æ®
            **kwargs: é¢å¤–å‚æ•°

        Returns:
            dict: æ›´æ–°ç»“æœ
        """
        result = {
            'success': False,
            'affected_rows': 0,
            'error': None
        }

        try:
            with self.transaction(auto_retry=True) as session:
                query = session.query(model_class)

                # åº”ç”¨è¿‡æ»¤æ¡ä»¶
                for key, value in filter_conditions.items():
                    if hasattr(model_class, key):
                        query = query.filter(getattr(model_class, key) == value)

                # æ‰§è¡Œæ›´æ–°
                affected_rows = query.update(update_data, synchronize_session=False)

                result['success'] = True
                result['affected_rows'] = affected_rows
                result['updated_count'] = affected_rows

        except Exception as e:
            result['error'] = str(e)
            logger.error(f"åŸå­æ›´æ–°å¤±è´¥: {e}")

        return result

    def batch_operations(self, operations, batch_size=100):
        """æ‰¹é‡æ“ä½œç®¡ç†

        Args:
            operations: æ“ä½œåˆ—è¡¨
            batch_size: æ‰¹æ¬¡å¤§å°

        Returns:
            dict: æ‰¹é‡æ“ä½œç»“æœ
        """
        result = {
            'total_operations': len(operations),
            'success_count': 0,
            'failed_count': 0,
            'failed_operations': []
        }

        try:
            with self.transaction(auto_retry=True) as session:
                for i in range(0, len(operations), batch_size):
                    batch = operations[i:i + batch_size]

                    for operation in batch:
                        try:
                            operation['func'](session, *operation.get('args', []), **operation.get('kwargs', {}))
                            result['success_count'] += 1
                        except Exception as e:
                            result['failed_count'] += 1
                            result['failed_operations'].append({
                                'operation': operation,
                                'error': str(e)
                            })

        except Exception as e:
            logger.error(f"æ‰¹é‡æ“ä½œæ‰§è¡Œå¤±è´¥: {e}")
            result['batch_error'] = str(e)

        return result


class GracefulShutdownManager:
    """ä¼˜é›…å…³é—­ç®¡ç†å™¨

    è´Ÿè´£ç³»ç»Ÿçš„ä¼˜é›…å…³é—­ï¼š
    1. ä¿¡å·å¤„ç†
    2. èµ„æºæ¸…ç†
    3. ä»»åŠ¡å®Œæˆç­‰å¾…
    4. çŠ¶æ€ä¿å­˜
    """

    def __init__(self):
        self.shutdown_requested = threading.Event()
        self.active_connections = set()
        self.active_tasks = set()
        self.shutdown_timeout = 30  # 30ç§’å…³é—­è¶…æ—¶
        self.cleanup_handlers = []

    def register_cleanup_handler(self, handler):
        """æ³¨å†Œæ¸…ç†å¤„ç†å™¨"""
        self.cleanup_handlers.append(handler)

    def register_active_connection(self, connection_id):
        """æ³¨å†Œæ´»è·ƒè¿æ¥"""
        self.active_connections.add(connection_id)

    def unregister_active_connection(self, connection_id):
        """å–æ¶ˆæ³¨å†Œæ´»è·ƒè¿æ¥"""
        self.active_connections.discard(connection_id)

    def register_active_task(self, task_id):
        """æ³¨å†Œæ´»è·ƒä»»åŠ¡"""
        self.active_tasks.add(task_id)

    def unregister_active_task(self, task_id):
        """å–æ¶ˆæ³¨å†Œæ´»è·ƒä»»åŠ¡"""
        self.active_tasks.discard(task_id)

    def is_shutdown_requested(self):
        """æ£€æŸ¥æ˜¯å¦è¯·æ±‚å…³é—­"""
        return self.shutdown_requested.is_set()

    @contextmanager
    def with_shutdown_protection(self, operation_name=None):
        """å…³é—­ä¿æŠ¤ä¸Šä¸‹æ–‡ç®¡ç†å™¨"""
        if self.is_shutdown_requested():
            raise RuntimeError(f"ç³»ç»Ÿæ­£åœ¨å…³é—­ï¼Œæ— æ³•æ‰§è¡Œæ“ä½œ: {operation_name}")

        try:
            yield
        except Exception as e:
            logger.error(f"æ“ä½œ {operation_name} æ‰§è¡Œå¤±è´¥: {e}")
            raise

    def wait_for_tasks_completion(self, timeout=None):
        """ç­‰å¾…æ´»è·ƒä»»åŠ¡å®Œæˆ"""
        timeout = timeout or self.shutdown_timeout
        start_time = time.time()

        while self.active_tasks:
            if time.time() - start_time > timeout:
                logger.warning(f"ç­‰å¾…ä»»åŠ¡å®Œæˆè¶…æ—¶ï¼Œå¼ºåˆ¶å…³é—­ã€‚å‰©ä½™ä»»åŠ¡: {len(self.active_tasks)}")
                break

            logger.info(f"ç­‰å¾… {len(self.active_tasks)} ä¸ªä»»åŠ¡å®Œæˆ...")
            time.sleep(1)

    def perform_cleanup(self):
        """æ‰§è¡Œæ¸…ç†æ“ä½œ"""
        logger.info("å¼€å§‹æ‰§è¡Œç³»ç»Ÿæ¸…ç†...")

        for handler in self.cleanup_handlers:
            try:
                if callable(handler):
                    handler()
                logger.info("æ¸…ç†å¤„ç†å™¨æ‰§è¡ŒæˆåŠŸ")
            except Exception as e:
                logger.error(f"æ¸…ç†å¤„ç†å™¨æ‰§è¡Œå¤±è´¥: {e}")

        # ç­‰å¾…è¿æ¥å…³é—­
        if self.active_connections:
            logger.info(f"ç­‰å¾… {len(self.active_connections)} ä¸ªè¿æ¥å…³é—­...")
            # è¿™é‡Œå¯ä»¥æ·»åŠ è¿æ¥å…³é—­é€»è¾‘

        logger.info("ç³»ç»Ÿæ¸…ç†å®Œæˆ")

    def setup_signal_handlers(self):
        """è®¾ç½®ä¿¡å·å¤„ç†å™¨"""
        import signal

        def signal_handler(signum, frame):
            logger.info(f"æ”¶åˆ°ä¿¡å· {signum}ï¼Œå¼€å§‹ä¼˜é›…å…³é—­...")
            self.shutdown_requested.set()

        # æ³¨å†Œä¿¡å·å¤„ç†å™¨
        signal.signal(signal.SIGINT, signal_handler)  # Ctrl+C
        signal.signal(signal.SIGTERM, signal_handler)  # ç»ˆæ­¢ä¿¡å·

        try:
            signal.signal(signal.SIGBREAK, signal_handler)  # Windowsä¸‹çš„ä¸­æ–­ä¿¡å·
        except:
            pass  # åœ¨éWindowsç³»ç»Ÿä¸Šå¿½ç•¥

    def graceful_shutdown(self):
        """æ‰§è¡Œä¼˜é›…å…³é—­"""
        logger.info("å¼€å§‹ä¼˜é›…å…³é—­...")

        # è®¾ç½®å…³é—­æ ‡å¿—
        self.shutdown_requested.set()

        # ç­‰å¾…ä»»åŠ¡å®Œæˆ
        self.wait_for_tasks_completion()

        # æ‰§è¡Œæ¸…ç†
        self.perform_cleanup()

        logger.info("ä¼˜é›…å…³é—­å®Œæˆ")


# å…¨å±€ç®¡ç†å™¨å®ä¾‹
thread_manager = ThreadSafeManager()
transaction_manager = TransactionManager()
graceful_shutdown_manager = GracefulShutdownManager()


# ==========================================
# 4. è¾…åŠ©å·¥å…·å‡½æ•°
# ==========================================

def smart_truncate(content, max_length=3000):
    if not content: return ""
    # ç¡®ä¿å†…å®¹æ˜¯å­—ç¬¦ä¸²ç±»å‹
    if not isinstance(content, str):
        content = str(content)
    
    # å¤„ç†ç¼–ç é—®é¢˜ï¼Œç¡®ä¿æ˜¯æœ‰æ•ˆçš„UTF-8
    try:
        # å¦‚æœæ˜¯bytesï¼Œå…ˆè§£ç 
        if isinstance(content, bytes):
            content = content.decode('utf-8', errors='replace')
        # é‡æ–°ç¼–ç ç¡®ä¿æ²¡æœ‰æ— æ•ˆå­—ç¬¦
        content = content.encode('utf-8', errors='replace').decode('utf-8')
    except Exception as e:
        logger.warning(f"å†…å®¹ç¼–ç å¤„ç†è­¦å‘Š: {e}")
        content = str(content)
    
    # ç§»é™¤æ§åˆ¶å­—ç¬¦ï¼Œé˜²æ­¢æ•°æ®åº“é”™è¯¯
    import re
    # ç§»é™¤ASCIIæ§åˆ¶å­—ç¬¦(0-31)å’ŒDEL(127)ï¼Œä½†ä¿ç•™æ¢è¡Œç¬¦(10)å’Œåˆ¶è¡¨ç¬¦(9)
    content = re.sub(r'[\x00-\x08\x0B\x0C\x0E-\x1F\x7F]', '', content)
    
    if len(content) <= max_length: 
        return content
    
    half = max_length // 2
    truncated = content[:half] + f"\n\n...[ä¸­é—´çœç•¥ {len(content) - max_length} å­—]\n\n" + content[-half:]
    
    # å†æ¬¡ç¡®ä¿æˆªæ–­åçš„å†…å®¹ç¼–ç æ­£ç¡®
    try:
        truncated = truncated.encode('utf-8', errors='replace').decode('utf-8')
    except Exception as e:
        logger.warning(f"æˆªæ–­åå†…å®¹ç¼–ç å¤„ç†è­¦å‘Š: {e}")
        truncated = str(truncated)
    
    return truncated


def calculate_hash(content):
    if not content: return ""
    return hashlib.md5(str(content).encode('utf-8')).hexdigest()


def get_external_files_from_api(project_id, task_id=None):
    """ä»å®é™…APIè·å–æŒ‡å®šé¡¹ç›®çš„æ–‡æ¡£ä¿¡æ¯

    Args:
        project_id: é¡¹ç›®ID
        task_id: ä»»åŠ¡IDï¼ˆå¯é€‰ï¼‰
    """
    try:
        logger.info(f"   ğŸ”„ æ­£åœ¨ä»APIè·å–é¡¹ç›® {project_id} çš„æ–‡æ¡£ä¿¡æ¯...")

        # æ„å»ºè¯·æ±‚payloadï¼Œæ ¹æ®æ–°æ¥å£è§„èŒƒåŒ…å«task_id
        payload = {"project_id": project_id}
        if task_id:
            payload["task_id"] = task_id

        response = requests.post(config.GET_FILES_API, json=payload, timeout=config.API_TIMEOUT)
        response.raise_for_status()

        data = response.json()
        if isinstance(data, dict) and data.get('code') == 0:
            files_data = data.get('data', [])
            logger.info(f"   âœ… æˆåŠŸè·å–é¡¹ç›® {project_id} çš„ {len(files_data)} ä¸ªæ–‡æ¡£é¡¹")
            return files_data
        else:
            logger.error(f"   âŒ APIè¿”å›é”™è¯¯: {data}")
            return []
    except Exception as e:
        logger.error(f"   âŒ è·å–æ–‡æ¡£ä¿¡æ¯å¼‚å¸¸: {e}")
        return []


def get_external_files_from_json(project_id):
    """Mockæ•°æ®å…œåº•"""
    json_filename = "mock_get_file_response.json"
    if not os.path.exists(json_filename): return []
    try:
        with open(json_filename, 'r', encoding='utf-8') as f:
            data = json.load(f)
            if isinstance(data, dict) and str(data.get('code')) == '200':
                return data.get('data', [])
            return data if isinstance(data, list) else []
    except:
        pass
    return []


def get_external_files_from_all_sources(project_id, task_id=None):
    """ä»æ‰€æœ‰æ¥æºè·å–é¡¹ç›®æ–‡ä»¶ä¿¡æ¯

    Args:
        project_id: é¡¹ç›®ID
        task_id: ä»»åŠ¡IDï¼ˆå¯é€‰ï¼‰
    """
    if config.USE_REAL_API:
        api_data = get_external_files_from_api(project_id, task_id)
        if api_data: return api_data
        logger.warning(f"   âš ï¸ APIå¤±è´¥ï¼Œä½¿ç”¨mockæ•°æ®ä½œä¸ºå¤‡ç”¨")
    return get_external_files_from_json(project_id)


def get_project_infos_from_api():
    try:
        logger.info("   ğŸ”„ æ­£åœ¨ä»APIè·å–é¡¹ç›®ä¿¡æ¯...")
        response = requests.post(config.GET_PROJECTS_API, timeout=config.API_TIMEOUT)
        response.raise_for_status()
        data = response.json()
        if isinstance(data, dict) and data.get('code') == 0:
            return data.get('data', [])
    except Exception as e:
        logger.error(f"   âŒ è·å–é¡¹ç›®ä¿¡æ¯å¼‚å¸¸: {e}")
    return []


def get_project_infos_from_json():
    json_filename = "mock_get_project_response.json"
    if not os.path.exists(json_filename): return []
    try:
        with open(json_filename, 'r', encoding='utf-8') as f:
            data = json.load(f)
            if isinstance(data, dict) and str(data.get('code')) == '200': return data.get('data', [])
            return data if isinstance(data, list) else []
    except:
        pass
    return []


if config.USE_REAL_API:
    MOCK_EXTERNAL_PROJECTS = get_project_infos_from_api() or get_project_infos_from_json()
else:
    MOCK_EXTERNAL_PROJECTS = get_project_infos_from_json()


# ==========================================
# 5. æ ¸å¿ƒä¸šåŠ¡é€»è¾‘
# ==========================================

def async_parse_file(file_info):
    """ å•ä¸ªæ–‡ä»¶è§£æä»»åŠ¡ """
    file_path = file_info.get('file_path')
    file_name = file_info.get('file_name')
    # ä¸éœ€è¦ thread_name å˜é‡ï¼Œlogging æ¨¡å—ä¼šè‡ªåŠ¨è·å–

    logger.info(f"ğŸ”„ å¼€å§‹å¼‚æ­¥è§£ææ–‡ä»¶: {file_name}")

    try:
        content = doc_parser.extract_content(file_path, file_info.get('file_type'),
                                             file_name) if HAS_PARSER else f"[Mock Content for {file_name}]"

        if content:
            logger.info(f"âœ… æ–‡ä»¶è§£ææˆåŠŸ: {file_name} (å†…å®¹é•¿åº¦: {len(content)})")
        else:
            logger.warning(f"âš ï¸ æ–‡ä»¶è§£ææˆåŠŸä½†å†…å®¹ä¸ºç©º: {file_name}")

        return (file_info, content, None)
    except Exception as e:
        logger.error(f"âŒ æ–‡ä»¶è§£æå¤±è´¥: {file_name} - {str(e)}", exc_info=True)
        return (file_info, None, str(e))


def core_sync_process(project_id, raw_files_data, task_id=None):
    """ åŒæ­¥æ–‡ä»¶ä¸è§„åˆ™ """
    task_info = f"[ä»»åŠ¡:{task_id}] " if task_id else ""
    logger.info(f"ğŸ“¥ [{project_id}] {task_info}å¼€å§‹åŒæ­¥æ•°æ®ä¸æå–è§„åˆ™...")

    project = db.session.get(Project, project_id)
    if not project:
        raise Exception("Project not found")

    # è·å–æˆ–åˆ›å»º ProjectEvaluation è®°å½•
    evaluation = ProjectEvaluation.query.filter_by(
        project_id=project_id,
        task_id=task_id or 'DEFAULT_TASK'
    ).first()

    if not evaluation:
        evaluation = ProjectEvaluation(
            project_id=project_id,
            task_id=task_id or 'DEFAULT_TASK',
            status='SYNCING'
        )
        db.session.add(evaluation)
    else:
        evaluation.status = 'SYNCING'
        # æ›´æ–°é‡æ–°è¿è¡Œä»»åŠ¡çš„æ—¶é—´æˆ³
        evaluation.updated_at = datetime.now()

    db.session.flush()

    extracted_rules = []
    files_to_process = []
    FILE_DOWNLOAD_BASE_URL = config.FILE_DOWNLOAD_BASE_URL
    FILE_DOWNLOAD_TOKEN = config.FILE_DOWNLOAD_TOKEN

    for idx, item in enumerate(raw_files_data):
        item_name = item.get('item_name', 'æœªå‘½åé¡¹')
        rule_info = {
            "æ£€æŸ¥ç»†é¡¹": item_name,
            "æ£€æŸ¥å­åˆ†ç±»": item.get('file_type_name', item_name),
            "åˆ†å€¼": item.get('scores', 0),
            "æ‰“åˆ†è¯´æ˜": item.get('score_remark', 'æ— æ˜ç¡®æ‰“åˆ†è¯´æ˜')
        }
        extracted_rules.append(rule_info)

        file_list = item.get('file_list') or item.get('item_files', [])

        for file_idx, f in enumerate(file_list):
            attach_id = f.get('attach_id')
            if attach_id:
                file_url = f"{FILE_DOWNLOAD_BASE_URL}/{attach_id}?token={FILE_DOWNLOAD_TOKEN}"
                file_name = f.get('attach_name', f'{attach_id}.{f.get("attach_suffix", "")}')
                file_type = f.get('attach_suffix', '')
            else:
                file_url = f.get('file_path', '')
                file_name = f.get('file_name', f.get('name', ''))
                file_type = f.get('file_type') or f.get('type', '')

            files_to_process.append({
                "cat_id": item.get('item_id', 'unknown'),
                "cat_name": item.get('file_type_name', item_name),  # ä½¿ç”¨file_type_nameä½œä¸ºåˆ†ç±»åç§°
                "file_type_id": item.get('file_type_id', ''),
                "f": f,
                "path": file_url,
                "name": file_name,
                "attach_id": attach_id
            })

    # ä¿å­˜è§„åˆ™é…ç½®åˆ° ProjectEvaluation è¡¨
    evaluation.rules_config = json.dumps(extracted_rules, ensure_ascii=False)
    evaluation.updated_at = datetime.now()
    logger.info(f"ğŸ“‹ [{project_id}] {task_info}è§„åˆ™é…ç½®å·²ä¿å­˜ï¼Œå…± {len(extracted_rules)} é¡¹è§„åˆ™")

    parse_tasks = []
    new_files_count = 0
    updated_files_count = 0

    for item in files_to_process:
        f_path, f_name = item['path'], item['name']
        if not f_path: continue

        curr_hash = calculate_hash(f_path + f_name)

        # æŸ¥è¯¢æ–‡ä»¶ï¼Œç°åœ¨åŒ…å« task_id æ¡ä»¶
        db_file = ProjectFile.query.filter_by(
            project_id=project_id,
            task_id=task_id or 'DEFAULT_TASK',
            category_id=item['cat_id'],
            file_name=f_name
        ).first()

        file_type = item['f'].get('attach_suffix', '') if item.get('attach_id') else (
                    item['f'].get('file_type') or item['f'].get('type', ''))

        if not db_file:
            new_files_count += 1
            db_file = ProjectFile(
                project_id=project_id,
                task_id=task_id or 'DEFAULT_TASK',
                category_id=item['cat_id'],
                category_name=item['cat_name'],
                file_name=f_name,
                file_url=f_path,
                file_type=file_type,
                file_hash=curr_hash,
                parsed_content=None
            )
            db.session.add(db_file)
            db.session.flush()
            parse_tasks.append({'file_info': item, 'db_id': db_file.id})

        elif db_file.file_hash != curr_hash or not db_file.parsed_content:
            updated_files_count += 1
            parse_tasks.append({'file_info': item, 'db_id': db_file.id})

    db.session.commit()
    logger.info(f"ğŸ“Š [{project_id}] {task_info}æ–‡ä»¶å…¥åº“å®Œæˆ - æ–°å¢: {new_files_count}, æ›´æ–°: {updated_files_count}")

    if parse_tasks:
        logger.info(f"ğŸ“‹ [{project_id}] {task_info}å¼€å§‹å¼‚æ­¥è§£æï¼Œå…± {len(parse_tasks)} ä¸ªä»»åŠ¡")
        future_to_id = {}
        for task in parse_tasks:
            info = task['file_info']
            file_type = info['f'].get('attach_suffix', '') if info.get('attach_id') else (
                        info['f'].get('file_type') or info['f'].get('type', ''))

            p_arg = {
                'file_path': info['path'], 'file_type': file_type,
                'file_name': info['name'], 'db_id': task['db_id']
            }
            future = doc_parser_executor.submit(async_parse_file, p_arg)
            future_to_id[future] = task['db_id']

        completed_count = 0
        error_count = 0
        for future in as_completed(future_to_id):
            p_arg, content, error = future.result()
            target_file = ProjectFile.query.get(p_arg['db_id'])
            if target_file:
                if error:
                    error_count += 1
                    target_file.parsed_content = f"[ERROR] {error}"
                    logger.error(f"âŒ è§£æå¤±è´¥: {p_arg.get('file_name')}")
                else:
                    completed_count += 1
                    # ä½¿ç”¨smart_truncateå‡½æ•°æˆªæ–­è¿‡é•¿çš„å†…å®¹ï¼Œé˜²æ­¢"Data too long for column 'parsed_content'"é”™è¯¯
                    # ä½¿ç”¨é»˜è®¤æˆªæ–­é•¿åº¦3000ï¼Œç¡®ä¿ä¸ä¼šè¶…è¿‡æ•°æ®åº“å­—æ®µé™åˆ¶
                    target_file.parsed_content = smart_truncate(content, max_length=3000)
                    target_file.update_time = datetime.now()

                if completed_count % 5 == 0: db.session.commit()

        db.session.commit()
        logger.info(f"ğŸ“‹ [{project_id}] {task_info}è§£ææ±‡æ€» - æˆåŠŸ: {completed_count}, å¤±è´¥: {error_count}")

    return True


def core_evaluate_process(project_id, task_id=None):
    """ AIè¯„æµ‹é€»è¾‘ """
    task_info = f"[ä»»åŠ¡:{task_id}] " if task_id else ""
    logger.info(f"ğŸ§  [{project_id}] {task_info}å¼€å§‹AIè¯„æµ‹...")

    # è·å–è¯„æµ‹è®°å½•
    evaluation = ProjectEvaluation.query.filter_by(
        project_id=project_id,
        task_id=task_id or 'DEFAULT_TASK'
    ).first()

    if not evaluation:
        raise Exception(f"æœªæ‰¾åˆ°é¡¹ç›® {project_id} ä»»åŠ¡ {task_id or 'DEFAULT_TASK'} çš„è¯„æµ‹è®°å½•")

    evaluation.status = 'EVALUATING'
    db.session.flush()

    # 1. è§„åˆ™å‡†å¤‡ - ç°åœ¨ä» ProjectEvaluation è¡¨è·å–
    api_rules = []
    if evaluation.rules_config:
        try:
            api_rules = json.loads(evaluation.rules_config)
        except Exception as e:
            logger.error(f"è§„åˆ™è§£æå¤±è´¥: {e}")

    # è¿™é‡Œç®€åŒ–äº†åŸæœ‰çš„æ··åˆè§„åˆ™é€»è¾‘ï¼Œå‡è®¾ api_rules ä¸ºä¸»
    rules_data = api_rules
    if not rules_data:
        # å°è¯•åŠ è½½Excelå…œåº•
        excel_path = config.CHECK_RULES_FILE
        if HAS_PANDAS and os.path.exists(excel_path):
            try:
                df = pd.read_excel(excel_path)
                rules_data = df.to_dict(orient='records')
                logger.info(f"ä½¿ç”¨ Excel å…œåº•è§„åˆ™: {len(rules_data)} æ¡")
            except Exception as e:
                logger.error(f"ExcelåŠ è½½å¤±è´¥: {e}")

    if not rules_data:
        raise Exception("æ— æœ‰æ•ˆè§„åˆ™ï¼Œæ— æ³•è¯„æµ‹")

    # 2. å‡†å¤‡ä¸Šä¸‹æ–‡ - ç°åœ¨æŒ‰ task_id æŸ¥è¯¢æ–‡ä»¶
    all_files = ProjectFile.query.filter_by(
        project_id=project_id,
        task_id=task_id or 'DEFAULT_TASK'
    ).all()
    files_content_by_cat = {}
    for f in all_files:
        if f.parsed_content:
            if f.category_name not in files_content_by_cat: files_content_by_cat[f.category_name] = ""
            # ä½¿ç”¨é»˜è®¤æˆªæ–­é•¿åº¦3000ï¼Œç¡®ä¿ä¸ä¼šè¶…è¿‡æ•°æ®åº“å­—æ®µé™åˆ¶
            content = smart_truncate(f.parsed_content, max_length=3000)
            files_content_by_cat[f.category_name] += f"\n=== æ–‡ä»¶ï¼š{f.file_name} ===\n{content}\n"

    # 3. åˆ†ç»„è¯„æµ‹
    rules_by_cat = {}
    for r in rules_data:
        cat = r.get('æ£€æŸ¥å­åˆ†ç±»', r.get('æ£€æŸ¥ç»†é¡¹'))
        if cat not in rules_by_cat: rules_by_cat[cat] = []
        rules_by_cat[cat].append(r)

    # è·å–æ–‡ä»¶ä¿¡æ¯ä»¥å»ºç«‹ item_id æ˜ å°„å…³ç³»
    item_id_mapping = {}
    try:
        logger.info(f"   ğŸ“‹ [{project_id}] {task_info}è·å–æ–‡ä»¶ä¿¡æ¯å»ºç«‹item_idæ˜ å°„...")
        files_data = get_external_files_from_all_sources(project_id, task_id)
        if files_data and len(files_data) > 0:
            for file_item in files_data:
                # ä»æ¥å£è¿”å›çš„æ•°æ®ä¸­å»ºç«‹æ£€æŸ¥ç»†é¡¹åˆ°item_idçš„æ˜ å°„
                item_name = file_item.get("item_name", "")
                file_type_name = file_item.get("file_type_name", "")
                real_item_id = file_item.get("item_id", "")

                if real_item_id and (item_name or file_type_name):
                    # ä¼˜å…ˆä½¿ç”¨item_nameï¼Œå¦‚æœä¸å­˜åœ¨åˆ™ä½¿ç”¨file_type_name
                    mapping_key = item_name if item_name else file_type_name
                    item_id_mapping[mapping_key] = real_item_id
                    logger.info(f"   âœ… å»ºç«‹æ˜ å°„: {mapping_key} -> {real_item_id}")
        else:
            logger.warning(f"   âš ï¸ æœªèƒ½è·å–æ–‡ä»¶ä¿¡æ¯ï¼Œå°†ç”Ÿæˆé»˜è®¤item_id")
    except Exception as e:
        logger.warning(f"   âš ï¸ è·å–æ–‡ä»¶ä¿¡æ¯å»ºç«‹æ˜ å°„å¤±è´¥: {e}")

    final_results = []

    for category, category_rules in rules_by_cat.items():
        logger.info(f"   ğŸ‘‰ è¯„æµ‹åˆ†ç±»: {category}")
        category_context = files_content_by_cat.get(category, "")

        if not category_context.strip():
            for r in category_rules:
                item_name = r['æ£€æŸ¥ç»†é¡¹']
                real_item_id = item_id_mapping.get(item_name, f"item_{len(final_results)}")
                final_results.append({
                    "item_id": real_item_id,
                    "item": item_name, "category": category, "score": 0, "maxScore": int(r['åˆ†å€¼']),
                    "isPass": False, "reason": "æœªæ£€ç´¢åˆ°ç›¸å…³è¯æ˜ææ–™"
                })
            continue

        simple_rules = [{"item": r['æ£€æŸ¥ç»†é¡¹'], "criteria": r['æ‰“åˆ†è¯´æ˜']} for r in category_rules]
        system_prompt = """ä½ æ˜¯ä¸€åå»ºç­‘å·¥ç¨‹åˆè§„å®¡æ ¸å‘˜ã€‚è¯·æ ¹æ®æä¾›çš„æ–‡ä»¶å†…å®¹å’Œè§„åˆ™è¿›è¡Œè¯„åˆ† .
        è¿”å›æ ¼å¼å¿…é¡»ä¸º JSON æ•°ç»„ï¼š[{"item": "è§„åˆ™å", "is_compliant": true/false, "score_logic": "ç†ç”±", "reason_detail":"ç†ç”±ç»†èŠ‚"}]
        è¯„åˆ†ç»“æœéœ€è¦å½’å±äºä¸‹åˆ—å‡ ç±»ï¼Œscore_logicç¦æ­¢è‡ªè¡Œå¢å‡æ–‡å­—ï¼Œè¯¦ç»†ç†ç”±å†™åœ¨reason_detailä¸­ï¼š
        â‘  is_compliant = false ; score_logic = "ææ–™å­˜åœ¨ï¼ŒæŒ‰è§„åˆ™è¯„ä¼°ï¼Œä¸ç¬¦åˆè¯„åˆ†è¦æ±‚" 
        â‘¡ is_compliant = true ; score_logic = "ææ–™å­˜åœ¨ï¼ŒæŒ‰è§„åˆ™è¯„ä¼°ï¼Œæ•´ä½“ç¬¦åˆè¯„åˆ†è¦æ±‚"
        â‘¢ is_compliant = true ; score_logic = "ææ–™å­˜åœ¨ï¼Œä½†è§„åˆ™æ ‡å‡†è¾ƒä¸ºæ¨¡ç³Šï¼Œä¸è¶³ä»¥æ ¹æ®ææ–™ä¿¡æ¯ç»™å‡ºæ˜ç¡®åˆ¤æ–­"  
        â‘£ is_compliant = false ; score_logic = "ç¼ºå°‘ç›¸å…³è¯æ˜ææ–™"
        """
        user_prompt = f"æ–‡ä»¶å†…å®¹ï¼š\n{category_context}\n\nè§„åˆ™ï¼š\n{json.dumps(simple_rules, ensure_ascii=False)}"

        try:
            batch_results = []
            if HAS_ANTHROPIC and config.ZHIPU_API_KEY:
                client = anthropic.Anthropic(api_key=config.ZHIPU_API_KEY, base_url=config.ZHIPU_BASE_URL)
                response = client.messages.create(
                    model=config.ZHIPU_MODEL, max_tokens=2000,
                    system=system_prompt, messages=[{"role": "user", "content": user_prompt}]
                )
                import re
                json_match = re.search(r'\[.*\]', response.content[0].text, re.DOTALL)
                if json_match:
                    batch_results = json.loads(json_match.group())
            else:
                time.sleep(0.5)  # Mock delay
                batch_results = [{"item": r['æ£€æŸ¥ç»†é¡¹'], "is_compliant": True, "score_logic": "Mock Pass (No AI Key)"}
                                 for r in category_rules]

            # ç»“æœæ˜ å°„
            rule_map = {r['æ£€æŸ¥ç»†é¡¹']: r for r in category_rules}
            for res in batch_results:
                target = res.get('item')
                if target in rule_map:
                    orig = rule_map[target]
                    item_name = orig['æ£€æŸ¥ç»†é¡¹']
                    real_item_id = item_id_mapping.get(item_name, f"item_{len(final_results)}")
                    is_pass = res.get('is_compliant', False)
                    score_logic = res.get('score_logic', '')
                    if (score_logic == "ææ–™å­˜åœ¨ï¼Œä½†è§„åˆ™æ ‡å‡†è¾ƒä¸ºæ¨¡ç³Šï¼Œä¸è¶³ä»¥æ ¹æ®ææ–™ä¿¡æ¯ç»™å‡ºæ˜ç¡®åˆ¤æ–­"):
                        is_pass = True
                        score_logic = "ææ–™å­˜åœ¨ï¼ŒæŒ‰è§„åˆ™è¯„ä¼°ï¼Œæ•´ä½“ç¬¦åˆè¯„åˆ†è¦æ±‚"
                    final_results.append({
                        "item_id": real_item_id,
                        "item": item_name, "category": category,
                        "score": int(orig['åˆ†å€¼']) if is_pass else 0,
                        "maxScore": int(orig['åˆ†å€¼']),
                        "isPass": is_pass,
                        "reason": score_logic
                    })
                    del rule_map[target]

            # è¡¥æ¼
            for r in rule_map.values():
                item_name = r['æ£€æŸ¥ç»†é¡¹']
                real_item_id = item_id_mapping.get(item_name, f"item_{len(final_results)}")
                final_results.append({
                    "item_id": real_item_id,
                    "item": item_name, "category": category,
                    "score": 0, "maxScore": int(r['åˆ†å€¼']),
                    "isPass": False, "reason": "æœªæ£€ç´¢åˆ°ç›¸å…³è¯æ˜ææ–™"
                })

        except Exception as e:
            logger.error(f"AIè¯„æµ‹å¼‚å¸¸: {e}", exc_info=True)
            for r in category_rules:
                item_name = r['æ£€æŸ¥ç»†é¡¹']
                real_item_id = item_id_mapping.get(item_name, f"item_{len(final_results)}")
                final_results.append({
                    "item_id": real_item_id,
                    "item": item_name, "score": 0, "maxScore": int(r['åˆ†å€¼']), "isPass": False,
                    "reason": f"ç³»ç»Ÿé”™è¯¯: {e}"
                })

    # ä»å·²æœ‰çš„ item_id_mapping ä¸­æå–çœŸå®çš„ check_date å’Œ check_person_name
    # å¦‚æœæˆ‘ä»¬åœ¨å»ºç«‹æ˜ å°„æ—¶æœ‰è·å–æ–‡ä»¶æ•°æ®ï¼Œä½¿ç”¨è¿™äº›æ•°æ®
    if not files_data:
        # å¦‚æœå‰é¢å»ºç«‹æ˜ å°„æ—¶æ²¡æœ‰è·å–åˆ°æ–‡ä»¶æ•°æ®ï¼Œé‡æ–°è·å–
        try:
            logger.info(f"   ğŸ“‹ [{project_id}] {task_info}è·å–æ–‡ä»¶ä¿¡æ¯ä¸­çš„çœŸå®æ•°æ®...")
            files_data = get_external_files_from_all_sources(project_id, task_id)
        except Exception as e:
            logger.warning(f"   âš ï¸ è·å–æ–‡ä»¶ä¿¡æ¯å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å€¼: {e}")
            files_data = []

    if files_data and len(files_data) > 0:
        # ä»æ¥å£è¿”å›çš„æ•°æ®ä¸­è·å–çœŸå®çš„ check_date å’Œ check_person_name
        # æ ¹æ®æ¥å£æ–‡æ¡£ï¼Œæ¯ä¸ªæ–‡ä»¶é¡¹éƒ½åŒ…å«è¿™äº›å­—æ®µ
        first_item = files_data[0]
        real_check_date = first_item.get("check_date")
        real_check_person_name = first_item.get("check_person_name")

        if real_check_date:
            evaluation.check_date = real_check_date
            logger.info(f"   âœ… ä¿å­˜çœŸå®æ£€æŸ¥æ—¥æœŸ: {real_check_date}")

        if real_check_person_name:
            evaluation.check_person_name = real_check_person_name
            logger.info(f"   âœ… ä¿å­˜çœŸå®æ£€æŸ¥äººå‘˜: {real_check_person_name}")
    else:
        logger.warning(f"   âš ï¸ æœªèƒ½è·å–æ–‡ä»¶ä¿¡æ¯ï¼Œå°†ä½¿ç”¨é»˜è®¤å€¼")

    # ä¿å­˜è¯„æµ‹ç»“æœåˆ° ProjectEvaluation è¡¨
    evaluation.evaluation_result = json.dumps(final_results, ensure_ascii=False)
    evaluation.status = 'COMPLETED'
    evaluation.updated_at = datetime.now()

    # åŒæ—¶æ›´æ–° Project è¡¨çš„ last_update æ—¶é—´æˆ³
    project = db.session.get(Project, project_id)
    if project:
        project.last_update = datetime.now()

    db.session.commit()
    logger.info(f"ğŸ [{project_id}] {task_info}è¯„æµ‹å®Œæˆ")


def async_full_pipeline_task(app_context, project_id, task_id=None):
    """ å…¨é“¾è·¯è‡ªåŠ¨åŒ–ä»»åŠ¡

    Args:
        app_context: Flaskåº”ç”¨ä¸Šä¸‹æ–‡
        project_id: é¡¹ç›®ID
        task_id: ä»»åŠ¡IDï¼ˆå¯é€‰ï¼‰
    """
    # å…³é”®ä¿®æ”¹ï¼šç§»é™¤ setup_thread_loggingï¼Œç›´æ¥ä½¿ç”¨å…¨å±€ logger
    # logger ä¼šè‡ªåŠ¨è®°å½• threadName
    task_info = f"ä»»åŠ¡ID: {task_id}, " if task_id else ""
    logger.info(f"ğŸš€ [{project_id}] å…¨é“¾è·¯è¯„æµ‹ä»»åŠ¡å¯åŠ¨... {task_info}")

    with app_context:
        try:
            raw_data = get_external_files_from_all_sources(project_id, task_id)
            if not raw_data:
                raise Exception("æ— æ³•è·å–é¡¹ç›®æ–‡ä»¶ä¿¡æ¯ (Mock Data Empty)")

            core_sync_process(project_id, raw_data, task_id)
            core_evaluate_process(project_id, task_id)

            # æ›´æ–° ProjectEvaluation çŠ¶æ€ä¸º COMPLETED
            evaluation = ProjectEvaluation.query.filter_by(
                project_id=project_id,
                task_id=task_id or 'DEFAULT_TASK'
            ).first()
            if evaluation:
                evaluation.status = 'COMPLETED'

            db.session.commit()
            logger.info(f"âœ… [{project_id}] {task_info}ä»»åŠ¡æˆåŠŸå®Œæˆ")

        except Exception as e:
            logger.error(f"âŒ [{project_id}] {task_info}ä»»åŠ¡ä¸­æ–­: {str(e)}", exc_info=True)

            # æ›´æ–° ProjectEvaluation çŠ¶æ€ä¸º ERROR
            evaluation = ProjectEvaluation.query.filter_by(
                project_id=project_id,
                task_id=task_id or 'DEFAULT_TASK'
            ).first()
            if evaluation:
                evaluation.status = 'ERROR'
                evaluation.evaluation_result = json.dumps(
                    [{"reason": f"æµç¨‹å¤±è´¥: {str(e)}", "item": "ç³»ç»Ÿé”™è¯¯", "isPass": False}])

            db.session.commit()


# ==========================================
# 5. å‰ç«¯è·¯ç”± (ç»Ÿä¸€æœåŠ¡æ¶æ„)
# ==========================================

@app.route('/')
def index():
    """å‰ç«¯é¦–é¡µ - é‡å®šå‘åˆ°é¡¹ç›®åˆ—è¡¨é¡µ"""
    return send_from_directory('static', 'project/frontend_improved.html')

@app.route('/project/<path:filename>')
def serve_project(filename):
    """é¡¹ç›®é¡µé¢è·¯ç”±"""
    return send_from_directory('static/project', filename)

@app.route('/<path:filename>')
def serve_static(filename):
    """é™æ€èµ„æºè·¯ç”±ï¼ˆconfig.js, config-manager.htmlç­‰ï¼‰"""
    return send_from_directory('static', filename)


# ==========================================
# 6. API æ¥å£
# ==========================================

@app.route('/api/task_statistics', methods=['GET'])
def api_task_statistics():
    """
    è·å–ä»»åŠ¡çŠ¶æ€ç»Ÿè®¡ä¿¡æ¯
    è¿”å›ï¼šé¡¹ç›®æ€»æ•°ã€è¿è¡Œä¸­ä»»åŠ¡ã€å·²å®Œæˆä»»åŠ¡ã€å¼‚å¸¸ä»»åŠ¡çš„æ•°é‡
    """
    try:
        # ç»Ÿè®¡å„ç§çŠ¶æ€çš„ä»»åŠ¡æ•°é‡
        task_stats = {}
        total_tasks = 0

        # å®šä¹‰æ‰€æœ‰å¯èƒ½çš„çŠ¶æ€
        all_statuses = ['IDLE', 'PENDING', 'SYNCING', 'EVALUATING', 'COMPLETED', 'ERROR', 'CANCELLED', 'PAUSED']

        for status in all_statuses:
            count = ProjectEvaluation.query.filter_by(status=status).count()
            task_stats[status.lower()] = count
            total_tasks += count

        # æ ¹æ®ç”¨æˆ·è¦æ±‚åˆ†ç±»ç»Ÿè®¡
        running_tasks = task_stats['syncing'] + task_stats['evaluating'] + task_stats['pending']
        completed_tasks = task_stats['completed']
        error_tasks = task_stats['error']

        # è·å–é¡¹ç›®æ€»æ•°
        total_projects = Project.query.count()

        response_data = {
            "total_projects": total_projects,
            "total_tasks": total_tasks,
            "running_tasks": running_tasks,
            "completed_tasks": completed_tasks,
            "error_tasks": error_tasks,
            "detailed_stats": task_stats
        }

        logger.info(f"ä»»åŠ¡ç»Ÿè®¡: é¡¹ç›®æ€»æ•°={total_projects}, ä»»åŠ¡æ€»æ•°={total_tasks}, è¿è¡Œä¸­={running_tasks}, å·²å®Œæˆ={completed_tasks}, å¼‚å¸¸={error_tasks}")

        return jsonify({"code": 0, "data": response_data})

    except Exception as e:
        logger.error(f"è·å–ä»»åŠ¡ç»Ÿè®¡å¤±è´¥: {e}")
        return jsonify({"code": 500, "message": f"è·å–ä»»åŠ¡ç»Ÿè®¡å¤±è´¥: {str(e)}"})


@app.route('/api/projects', methods=['GET'])
def api_projects():
    # å¢é‡åŒæ­¥é€»è¾‘
    try:
        logger.info("å¼€å§‹å¤„ç† /api/projects è¯·æ±‚")
        external_data = MOCK_EXTERNAL_PROJECTS
        logger.info(f"å¤–éƒ¨æ•°æ®æ•°é‡: {len(external_data)}")

        existing_projects = {p.id: p for p in Project.query.all()}
        logger.info(f"ç°æœ‰é¡¹ç›®æ•°é‡: {len(existing_projects)}")

        for ext_p in external_data:
            pid = ext_p['project_id']
            if pid in existing_projects:
                db_p = existing_projects[pid]
                db_p.project_name = ext_p['project_name']
                db_p.project_code = ext_p['project_code']
                # æ›´æ–°æ–°å¢å­—æ®µ
                db_p.epc_manager = ext_p.get('epc_manager')
                db_p.entrust_manager = ext_p.get('entrust_manager')
            else:
                new_p = Project(
                    id=pid,
                    project_code=ext_p['project_code'],
                    project_name=ext_p['project_name'],
                    epc_manager=ext_p.get('epc_manager'),
                    entrust_manager=ext_p.get('entrust_manager')
                )
                db.session.add(new_p)
        db.session.commit()

        logger.info("å¼€å§‹æŸ¥è¯¢é¡¹ç›®åˆ—è¡¨")
        # ç®€åŒ–æŸ¥è¯¢ï¼šè·å–é¡¹ç›®åˆ—è¡¨ï¼ˆæš‚æ—¶å»æ‰last_updateæ’åºä»¥é¿å…datetime2é—®é¢˜ï¼‰
        try:
            projects = Project.query.order_by(Project.last_update.desc()).all()
        except Exception as order_error:
            logger.warning(f"æŒ‰last_updateæ’åºå¤±è´¥: {order_error}ï¼Œæ”¹ä¸ºæŒ‰idæ’åº")
            projects = Project.query.order_by(Project.id.desc()).all()
        logger.info(f"æŸ¥è¯¢åˆ°é¡¹ç›®æ•°é‡: {len(projects)}")

        res = []
        for i, project in enumerate(projects):
            logger.info(f"å¤„ç†ç¬¬{i+1}ä¸ªé¡¹ç›®: {project.id}")
            # ç»Ÿè®¡æ¯ä¸ªé¡¹ç›®çš„ä»»åŠ¡æ•°é‡
            task_count = db.session.query(ProjectEvaluation).filter_by(project_id=project.id).count()

            res.append({
                "project_id": project.id,
                "project_name": project.project_name or "",
                "project_code": project.project_code or "",
                "epc_manager": project.epc_manager or "",  # é¡¹ç›®ç»ç†
                "entrust_manager": project.entrust_manager or "",  # é¡¹ç›®æ‰§è¡Œç»ç†
                "status": "IDLE",  # ç®€åŒ–çŠ¶æ€æ˜¾ç¤º
                "task_count": task_count,
                "last_update": safe_datetime_format(project.last_update)
            })

        logger.info("æˆåŠŸæ„å»ºå“åº”æ•°æ®")
        return jsonify({"code": 0, "data": res})
    except Exception as e:
        import traceback
        logger.error(f"API Error: {e}")
        logger.error(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
        return jsonify({"code": 500, "message": str(e)})


@app.route('/api/task/concurrency/status', methods=['GET'])
def get_task_concurrency_status():
    """
    è·å–å½“å‰ä»»åŠ¡å¹¶å‘çŠ¶æ€
    ç”¨äºå‰ç«¯æ˜¾ç¤ºå¹¶å‘é™åˆ¶å’Œå½“å‰è¿è¡Œä»»åŠ¡æ•°é‡
    """
    logger.info("ğŸ” æ”¶åˆ°å¹¶å‘çŠ¶æ€æŸ¥è¯¢è¯·æ±‚")

    try:
        with task_management_lock:
            status_info = check_task_concurrency_limit()

        # å¢åŠ é…ç½®ä¿¡æ¯
        status_info['config'] = {
            'max_concurrent_tasks': GLOBAL_CONFIG['MAX_CONCURRENT_TASKS'],
            'running_states': GLOBAL_CONFIG['RUNNING_STATES'],
            'rerunnable_states': GLOBAL_CONFIG['RERUNNABLE_STATES']
        }

        logger.info(f"âœ… å¹¶å‘çŠ¶æ€æŸ¥è¯¢æˆåŠŸ: {status_info}")

        return jsonify({
            "code": 0,
            "message": "è·å–å¹¶å‘çŠ¶æ€æˆåŠŸ",
            "data": status_info
        })

    except Exception as e:
        logger.error(f"âŒ è·å–å¹¶å‘çŠ¶æ€å¤±è´¥: {e}")
        return jsonify({
            "code": 500,
            "message": f"è·å–å¹¶å‘çŠ¶æ€å¤±è´¥: {str(e)}",
            "data": None
        })

@app.route('/api/start_evaluation', methods=['POST'])
def api_start_evaluation():
    try:
        # å‚æ•°éªŒè¯
        if not request.is_json:
            return jsonify({"code": 400, "message": "è¯·æ±‚å¿…é¡»ä¸ºJSONæ ¼å¼"})

        data = request.get_json()
        project_id = data.get('project_id')
        task_id = data.get('task_id')  # æ–°å¢task_idå‚æ•°ï¼ˆå¯é€‰ï¼‰

        # å¿…å¡«å‚æ•°æ£€æŸ¥
        if not project_id:
            return jsonify({"code": 400, "message": "é¡¹ç›®IDç¼ºå¤±"})

        # å¹¶å‘æ§åˆ¶æ£€æŸ¥
        with task_management_lock:
            concurrency_status = check_task_concurrency_limit()

        if not concurrency_status['is_allowed']:
            running_task_ids = [task['task_id'] for task in concurrency_status['running_tasks']]
            return jsonify({
                "code": 409,  # 409 Conflict è¡¨ç¤ºèµ„æºå†²çª
                "message": f"å·²è¾¾åˆ°æœ€å¤§å¹¶å‘ä»»åŠ¡æ•°é™åˆ¶ï¼ˆ{concurrency_status['max_count']}ä¸ªï¼‰ã€‚å½“å‰è¿è¡Œä»»åŠ¡ï¼š{', '.join(running_task_ids)}",
                "data": {
                    "current_count": concurrency_status['current_count'],
                    "max_count": concurrency_status['max_count'],
                    "running_tasks": concurrency_status['running_tasks']
                }
            })

        # å¯é€‰å‚æ•°task_idçš„æç¤ºï¼ˆå¦‚æœå°†æ¥éœ€è¦ï¼‰
        if task_id:
            logger.info(f"ğŸš€ å¯åŠ¨è¯„æµ‹ - é¡¹ç›®ID: {project_id}, ä»»åŠ¡ID: {task_id}")
        else:
            logger.info(f"ğŸš€ å¯åŠ¨è¯„æµ‹ - é¡¹ç›®ID: {project_id}")

        project = db.session.get(Project, project_id)
        if not project:
            return jsonify({"code": 500, "message": "é¡¹ç›®ä¸å­˜åœ¨æˆ–ç³»ç»Ÿé”™è¯¯"})

        # æ£€æŸ¥æ˜¯å¦æœ‰æ­£åœ¨è¿è¡Œçš„ä»»åŠ¡ï¼ˆæ”¯æŒtask_idéš”ç¦»ï¼‰
        evaluation = ProjectEvaluation.query.filter_by(
            project_id=project_id,
            task_id=task_id or 'DEFAULT_TASK'
        ).first()

        if evaluation and evaluation.status in ['SYNCING', 'EVALUATING']:
            task_desc = f"ä»»åŠ¡ {task_id}" if task_id else "é»˜è®¤ä»»åŠ¡"
            return jsonify({"code": 409, "message": "è¯¥é¡¹ç›®æ­£åœ¨è¿›è¡Œè¯„æµ‹ä¸­ï¼Œè¯·å‹¿é‡å¤æäº¤"})

        # åˆ›å»ºæˆ–æ›´æ–° ProjectEvaluation è®°å½•
        if not evaluation:
            evaluation = ProjectEvaluation(
                project_id=project_id,
                task_id=task_id or 'DEFAULT_TASK',
                status='EVALUATING'
            )
            db.session.add(evaluation)
        else:
            evaluation.status = 'EVALUATING'

        db.session.commit()

        # å¯åŠ¨çº¿ç¨‹ï¼Œä¼ é€’task_idï¼ˆå¦‚æœæä¾›ï¼‰
        thread = threading.Thread(target=async_full_pipeline_task, args=(app.app_context(), project_id, task_id))
        thread.start()

        return jsonify({"code": 200, "message": "è¯„æµ‹å·²å¯åŠ¨"})
    except Exception as e:
        logger.error(f"Start Error: {e}")
        return jsonify({"code": 500, "message": str(e)})


@app.route('/api/get_result', methods=['GET'])
def api_get_result():
    try:
        # å‚æ•°éªŒè¯
        project_id = request.args.get('project_id')
        task_id = request.args.get('task_id')  # task_idå‚æ•°ï¼ˆå¿…é€‰ï¼‰

        # å¿…å¡«å‚æ•°æ£€æŸ¥
        if not project_id:
            return jsonify({"code": 400, "message": "é¡¹ç›®IDç¼ºå¤±"})

        if not task_id:
            return jsonify({"code": 400, "message": "ä»»åŠ¡IDç¼ºå¤±"})

        # è®°å½•è¯·æ±‚ä¿¡æ¯ï¼ˆproject_idå’Œtask_idéƒ½æ˜¯å¿…é€‰å‚æ•°ï¼‰
        logger.info(f"ğŸ“Š æŸ¥è¯¢è¯„æµ‹ç»“æœ - é¡¹ç›®ID: {project_id}, ä»»åŠ¡ID: {task_id}")

        project = db.session.get(Project, project_id)
        if not project:
            return jsonify({"code": 500, "message": "é¡¹ç›®ä¸å­˜åœ¨æˆ–ç³»ç»Ÿé”™è¯¯"})

        # è·å–è¯„æµ‹è®°å½•ï¼ˆtask_idæ˜¯å¿…é€‰å‚æ•°ï¼‰
        evaluation = ProjectEvaluation.query.filter_by(
            project_id=project_id,
            task_id=task_id
        ).first()

        if not evaluation:
            return jsonify({"code": 500, "message": f"æœªæ‰¾åˆ°é¡¹ç›® {project_id} çš„ä»»åŠ¡ {task_id} è¯„æµ‹è®°å½•"})

        # ä»æ•°æ®åº“è¯»å–çœŸå®çš„check_dateå’Œcheck_person_name
        check_date = evaluation.check_date if evaluation.check_date else (evaluation.created_at.strftime("%Y-%m-%d") if evaluation.created_at else "")
        check_person_name = evaluation.check_person_name if evaluation.check_person_name else "AIè´¨æ£€å‘˜"

        logger.info(f"   âœ… ä»æ•°æ®åº“è·å–çœŸå®æ•°æ®: æ£€æŸ¥æ—¥æœŸ={check_date}, æ£€æŸ¥äººå‘˜={check_person_name}")

        # æ„å»ºå“åº”æ•°æ®ï¼Œä¸¥æ ¼æŒ‰ç…§æ–‡æ¡£è§„èŒƒ
        data = {
            "project_id": project.id,
            "project_code": project.project_code if project.project_code else "",
            "project_name": project.project_name if project.project_name else "",
            "epc_manager": project.epc_manager if project.epc_manager else "",  # é¡¹ç›®ç»ç†
            "entrust_manager": project.entrust_manager if project.entrust_manager else "",  # é¡¹ç›®æ‰§è¡Œç»ç†
            "check_date": check_date,
            "check_person_name": check_person_name,
            "status": evaluation.status,
            "last_update": evaluation.updated_at.strftime("%Y-%m-%d %H:%M:%S") if evaluation.updated_at else "",
            "evaluation_details": []
        }

        # è§£æè¯„æµ‹è¯¦æƒ…ï¼ˆä» ProjectEvaluation è¡¨è·å–ï¼‰
        if evaluation.evaluation_result:
            try:
                evaluation_data = json.loads(evaluation.evaluation_result)
                # ç¡®ä¿è¿”å›çš„æ˜¯æ•°ç»„æ ¼å¼ï¼Œå¹¶æŒ‰ç…§æ–‡æ¡£è¦æ±‚æ ¼å¼åŒ–æ¯ä¸ªæ¡ç›®
                if isinstance(evaluation_data, list):
                    formatted_details = []
                    for item in evaluation_data:
                        # æŸ¥è¯¢è¯¥æ£€æŸ¥ç»†é¡¹ç›¸å…³çš„æ–‡ä»¶
                        item_name = item.get("item", item.get("æ£€æŸ¥ç»†é¡¹", ""))
                        files_for_item = []

                        if item_name:
                            # æŸ¥è¯¢ä¸è¯¥æ£€æŸ¥ç»†é¡¹åç§°ç›¸å…³çš„æ–‡ä»¶
                            files = ProjectFile.query.filter_by(
                                project_id=project_id,
                                task_id=task_id
                            ).all()

                            files_for_item = [{"file_name": f.file_name, "file_type": f.file_type} for f in files if item_name in f.file_name]

                        # æŒ‰ç…§æ–‡æ¡£æ ¼å¼è¦æ±‚æ ‡å‡†åŒ–æ¯ä¸ªè¯„æµ‹é¡¹
                        formatted_item = {
                            "item_id": item.get("item_id", item.get("id", f"item_{len(formatted_details)}")),  # æ–°å¢item_idå­—æ®µ
                            "item": item.get("item", item.get("æ£€æŸ¥ç»†é¡¹", "æœªçŸ¥é¡¹ç›®")),
                            "category": item.get("category", item.get("æ£€æŸ¥å­åˆ†ç±»", "æœªåˆ†ç±»")),
                            "score": item.get("score", 0),
                            "maxScore": item.get("maxScore", item.get("åˆ†å€¼", 0)),
                            "isPass": item.get("isPass", item.get("is_compliant", False)),
                            "reason": item.get("reason", item.get("score_logic", item.get("æ‰“åˆ†è¯´æ˜", ""))),
                            "file_list": files_for_item
                        }
                        formatted_details.append(formatted_item)
                    data["evaluation_details"] = formatted_details
                elif isinstance(evaluation_data, dict):
                    # å¦‚æœæ˜¯å­—å…¸æ ¼å¼ï¼Œå°è¯•æå–ç›¸å…³å­—æ®µ
                    if "evaluation_details" in evaluation_data:
                        details_list = evaluation_data["evaluation_details"]
                        if isinstance(details_list, list):
                            formatted_details = []
                            for item in details_list:
                                formatted_item = {
                                    "item": item.get("item", item.get("æ£€æŸ¥ç»†é¡¹", "æœªçŸ¥é¡¹ç›®")),
                                    "category": item.get("category", item.get("æ£€æŸ¥å­åˆ†ç±»", "æœªåˆ†ç±»")),
                                    "score": item.get("score", 0),
                                    "maxScore": item.get("maxScore", item.get("åˆ†å€¼", 0)),
                                    "isPass": item.get("isPass", item.get("is_compliant", False)),
                                    "reason": item.get("reason", item.get("score_logic", item.get("æ‰“åˆ†è¯´æ˜", "")))
                                }
                                formatted_details.append(formatted_item)
                            data["evaluation_details"] = formatted_details
                        else:
                            # å•ä¸ªé¡¹ç›®
                            item = details_list
                            formatted_item = {
                                "item_id": item.get("item_id", item.get("id", f"item_{len(formatted_details)}")),  # æ–°å¢item_idå­—æ®µ
                                "item": item.get("item", item.get("æ£€æŸ¥ç»†é¡¹", "æœªçŸ¥é¡¹ç›®")),
                                "category": item.get("category", item.get("æ£€æŸ¥å­åˆ†ç±»", "æœªåˆ†ç±»")),
                                "score": item.get("score", 0),
                                "maxScore": item.get("maxScore", item.get("åˆ†å€¼", 0)),
                                "isPass": item.get("isPass", item.get("is_compliant", False)),
                                "reason": item.get("reason", item.get("score_logic", item.get("æ‰“åˆ†è¯´æ˜", "")))
                            }
                            data["evaluation_details"] = [formatted_item]
                    else:
                        # å°†å­—å…¸è½¬æ¢ä¸ºå•æ¡è®°å½•
                        formatted_item = {
                            "item_id": evaluation_data.get("item_id", evaluation_data.get("id", "item_0")),  # æ–°å¢item_idå­—æ®µ
                            "item": evaluation_data.get("item", evaluation_data.get("æ£€æŸ¥ç»†é¡¹", "æœªçŸ¥é¡¹ç›®")),
                            "category": evaluation_data.get("category", evaluation_data.get("æ£€æŸ¥å­åˆ†ç±»", "æœªåˆ†ç±»")),
                            "score": evaluation_data.get("score", 0),
                            "maxScore": evaluation_data.get("maxScore", evaluation_data.get("åˆ†å€¼", 0)),
                            "isPass": evaluation_data.get("isPass", evaluation_data.get("is_compliant", False)),
                            "reason": evaluation_data.get("reason", evaluation_data.get("score_logic", evaluation_data.get("æ‰“åˆ†è¯´æ˜", "")))
                        }
                        data["evaluation_details"] = [formatted_item]
            except json.JSONDecodeError as e:
                logger.warning(f"è¯„æµ‹ç»“æœJSONè§£æå¤±è´¥: {e}")
                data["evaluation_details"] = []

        return jsonify({"code": 200, "data": data})
    except Exception as e:
        logger.error(f"Get Result Error: {e}")
        return jsonify({"code": 500, "message": str(e)})


def reset_stuck_tasks():
    """ä½¿ç”¨æ™ºèƒ½ä»»åŠ¡æ¢å¤ç®¡ç†å™¨æ›¿æ¢ç®€å•ç²—æš´çš„é‡ç½®é€»è¾‘"""
    with app.app_context():
        try:
            # åˆ›å»ºæ™ºèƒ½ä»»åŠ¡æ¢å¤ç®¡ç†å™¨å®ä¾‹
            recovery_manager = TaskRecoveryManager()

            # æ‰§è¡Œæ™ºèƒ½ä»»åŠ¡æ¢å¤
            recovery_stats = recovery_manager.recover_stuck_tasks()

            # è®°å½•æ¢å¤ç»Ÿè®¡ä¿¡æ¯
            total_tasks = recovery_stats['total_checked']
            if total_tasks > 0:
                logger.info(f"ğŸ¯ æ™ºèƒ½ä»»åŠ¡æ¢å¤å®Œæˆ:")
                logger.info(f"   â€¢ æ€»æ£€æŸ¥ä»»åŠ¡æ•°: {total_tasks}")
                logger.info(f"   â€¢ å¯æ¢å¤ä»»åŠ¡æ•°: {recovery_stats['recoverable_tasks']}")
                logger.info(f"   â€¢ æˆåŠŸæ¢å¤ä»»åŠ¡æ•°: {recovery_stats['recovered_tasks']}")
                logger.info(f"   â€¢ å¤±è´¥ä»»åŠ¡æ•°: {recovery_stats['failed_tasks']}")
                logger.info(f"   â€¢ å¿½ç•¥ä»»åŠ¡æ•°: {recovery_stats['ignored_tasks']}")
            else:
                logger.info("âœ… ç³»ç»Ÿå¯åŠ¨æ£€æŸ¥ï¼šæ— å¡ä½ä»»åŠ¡")

        except Exception as e:
            logger.error(f"âŒ æ™ºèƒ½ä»»åŠ¡æ¢å¤å¤±è´¥ï¼Œå›é€€åˆ°åŸºç¡€é‡ç½®: {e}")

            # å›é€€åˆ°åŸºç¡€é‡ç½®é€»è¾‘
            try:
                stuck_evaluations = ProjectEvaluation.query.filter(
                    ProjectEvaluation.status.in_(['SYNCING', 'EVALUATING'])
                ).all()

                for evaluation in stuck_evaluations:
                    evaluation.status = 'ERROR'
                    evaluation.evaluation_result = json.dumps([{
                        "reason": "æœåŠ¡é‡å¯ï¼Œä»»åŠ¡è¢«é‡ç½®ï¼ˆå›é€€æ¨¡å¼ï¼‰",
                        "item": "ç³»ç»Ÿé”™è¯¯",
                        "isPass": False
                    }])

                if stuck_evaluations:
                    db.session.commit()
                    logger.info(f"ğŸ”„ å›é€€æ¨¡å¼ï¼šé‡ç½®äº† {len(stuck_evaluations)} ä¸ªå¡ä½çš„ä»»åŠ¡")

            except Exception as fallback_error:
                logger.error(f"âŒ å›é€€é‡ç½®ä¹Ÿå¤±è´¥: {fallback_error}")


@app.route('/api/projects/<project_id>/tasks', methods=['GET'])
def get_project_tasks(project_id):
    """è·å–æŒ‡å®šé¡¹ç›®çš„æ‰€æœ‰ä»»åŠ¡å†å²è®°å½•"""
    try:
        # å‚æ•°éªŒè¯
        if not project_id:
            return jsonify({
                "code": 400,
                "message": "é¡¹ç›®IDä¸èƒ½ä¸ºç©º",
                "data": None
            }), 400

        # åˆ†é¡µå‚æ•°
        page = int(request.args.get('page', 1))
        per_page = min(int(request.args.get('per_page', 20)), 100)  # é™åˆ¶æœ€å¤§100æ¡
        status_filter = request.args.get('status', '')

        logger.info(f"è·å–é¡¹ç›®ä»»åŠ¡å†å²: project_id={project_id}, page={page}, per_page={per_page}")

        # æ„å»ºæŸ¥è¯¢
        query = ProjectEvaluation.query.filter_by(project_id=project_id)

        # çŠ¶æ€ç­›é€‰
        if status_filter and status_filter != 'all':
            query = query.filter_by(status=status_filter.upper())

        # æŒ‰åˆ›å»ºæ—¶é—´å€’åºæ’åˆ—
        query = query.order_by(ProjectEvaluation.created_at.desc())

        # åˆ†é¡µæŸ¥è¯¢
        pagination = query.paginate(page=page, per_page=per_page, error_out=False)

        # æ ¼å¼åŒ–ä»»åŠ¡æ•°æ®
        tasks = []
        for task in pagination.items:
            # è§£æè¯„æµ‹ç»“æœ
            evaluation_result = []
            if task.evaluation_result:
                try:
                    evaluation_result = json.loads(task.evaluation_result) if isinstance(task.evaluation_result, str) else task.evaluation_result
                except json.JSONDecodeError:
                    evaluation_result = [{"item": "è§£æé”™è¯¯", "reason": "è¯„æµ‹ç»“æœæ ¼å¼é”™è¯¯", "isPass": False}]

            # è®¡ç®—é€šè¿‡ç‡
            total_items = len(evaluation_result)
            passed_items = sum(1 for item in evaluation_result if item.get('isPass', False))
            pass_rate = (passed_items / total_items * 100) if total_items > 0 else 0

            # è·å–æ–‡ä»¶æ•°é‡ï¼ˆæŒ‰ä»»åŠ¡ç»Ÿè®¡ï¼‰
            file_count = ProjectFile.query.filter_by(project_id=project_id, task_id=task.task_id or 'DEFAULT_TASK').count()

            tasks.append({
                "id": task.id,
                "task_id": task.task_id or "é»˜è®¤ä»»åŠ¡",
                "status": task.status,
                "file_count": file_count,
                "pass_rate": round(pass_rate, 1),
                "total_items": total_items,
                "passed_items": passed_items,
                "check_person_name": task.check_person_name,  # æ–°å¢ï¼šä»»åŠ¡å‘èµ·äºº
                "created_at": task.created_at.strftime('%Y-%m-%d %H:%M:%S'),
                "updated_at": task.updated_at.strftime('%Y-%m-%d %H:%M:%S'),
                "has_result": bool(evaluation_result),
                "result_preview": evaluation_result[:3] if evaluation_result else []  # å‰3ä¸ªç»“æœä½œä¸ºé¢„è§ˆ
            })

        # è·å–é¡¹ç›®åŸºæœ¬ä¿¡æ¯
        project = db.session.get(Project, project_id)
        project_info = None
        if project:
            # å®‰å…¨çš„datetimeæ ¼å¼åŒ–å¤„ç†
            last_update_str = None
            if project.last_update:
                try:
                    if isinstance(project.last_update, str):
                        last_update_str = project.last_update
                    elif hasattr(project.last_update, 'strftime'):
                        last_update_str = project.last_update.strftime('%Y-%m-%d %H:%M:%S')
                    else:
                        last_update_str = str(project.last_update)
                except Exception as date_error:
                    logger.warning(f"æ—¥æœŸæ ¼å¼åŒ–å¤±è´¥ project_id={project.id}: {date_error}")
                    last_update_str = None

            project_info = {
                "project_id": project.id,
                "project_name": project.project_name,
                "project_code": project.project_code,
                "epc_manager": project.epc_manager,  # é¡¹ç›®ç»ç†
                "entrust_manager": project.entrust_manager,  # é¡¹ç›®æ‰§è¡Œç»ç†
                "last_update": last_update_str
            }

        # æ„å»ºå“åº”
        response_data = {
            "project": project_info,
            "tasks": tasks,
            "pagination": {
                "current_page": page,
                "total_pages": pagination.pages,
                "total_items": pagination.total,
                "per_page": per_page,
                "has_next": pagination.has_next,
                "has_prev": pagination.has_prev
            }
        }

        logger.info(f"æˆåŠŸè·å–é¡¹ç›®ä»»åŠ¡å†å²: {len(tasks)} æ¡è®°å½•")

        return jsonify({
            "code": 0,
            "message": "è·å–é¡¹ç›®ä»»åŠ¡å†å²æˆåŠŸ",
            "data": response_data
        })

    except Exception as e:
        logger.error(f"è·å–é¡¹ç›®ä»»åŠ¡å†å²å¤±è´¥: {str(e)}")
        return jsonify({
            "code": 500,
            "message": f"è·å–é¡¹ç›®ä»»åŠ¡å†å²å¤±è´¥: {str(e)}",
            "data": None
        }), 500


@app.route('/api/projects/<project_id>/stats', methods=['GET'])
def get_project_stats(project_id):
    """è·å–æŒ‡å®šé¡¹ç›®çš„ç»Ÿè®¡ä¿¡æ¯"""
    try:
        # å‚æ•°éªŒè¯
        if not project_id:
            return jsonify({
                "code": 400,
                "message": "é¡¹ç›®IDä¸èƒ½ä¸ºç©º",
                "data": None
            }), 400

        logger.info(f"è·å–é¡¹ç›®ç»Ÿè®¡ä¿¡æ¯: project_id={project_id}")

        # è·å–é¡¹ç›®åŸºæœ¬ä¿¡æ¯
        project = db.session.get(Project, project_id)
        if not project:
            return jsonify({
                "code": 404,
                "message": "é¡¹ç›®ä¸å­˜åœ¨",
                "data": None
            }), 404

        # è·å–ä»»åŠ¡ç»Ÿè®¡
        total_tasks = ProjectEvaluation.query.filter_by(project_id=project_id).count()

        # çŠ¶æ€ç»Ÿè®¡
        status_counts = {}
        for status in ['IDLE', 'PENDING', 'SYNCING', 'EVALUATING', 'COMPLETED', 'ERROR', 'CANCELLED', 'PAUSED']:
            count = ProjectEvaluation.query.filter_by(project_id=project_id, status=status).count()
            status_counts[status.lower()] = count

        # è®¡ç®—æˆåŠŸç‡ï¼ˆåŸºäºå·²å®Œæˆçš„ä»»åŠ¡ï¼‰
        completed_tasks = status_counts['completed']
        successful_tasks = 0

        if completed_tasks > 0:
            # ç»Ÿè®¡é€šè¿‡ç‡>80%çš„ä»»åŠ¡æ•°é‡
            completed_evaluations = ProjectEvaluation.query.filter_by(
                project_id=project_id,
                status='COMPLETED'
            ).all()

            for eval_record in completed_evaluations:
                try:
                    if eval_record.evaluation_result:
                        result_data = json.loads(eval_record.evaluation_result) if isinstance(eval_record.evaluation_result, str) else eval_record.evaluation_result
                        if isinstance(result_data, list) and len(result_data) > 0:
                            passed_items = sum(1 for item in result_data if item.get('isPass', False))
                            pass_rate = (passed_items / len(result_data)) * 100
                            if pass_rate >= 80:
                                successful_tasks += 1
                except (json.JSONDecodeError, TypeError):
                    pass

        success_rate = (successful_tasks / completed_tasks * 100) if completed_tasks > 0 else 0

        # æœ€è¿‘ä»»åŠ¡ä¿¡æ¯
        recent_task = ProjectEvaluation.query.filter_by(project_id=project_id).order_by(
            ProjectEvaluation.updated_at.desc()
        ).first()

        # æ–‡ä»¶ç»Ÿè®¡
        file_count = ProjectFile.query.filter_by(project_id=project_id).count()

        # æ—¶é—´ç»Ÿè®¡
        first_task = ProjectEvaluation.query.filter_by(project_id=project_id).order_by(
            ProjectEvaluation.created_at.asc()
        ).first()

        stats_data = {
            "project_info": {
                "project_id": project.id,
                "project_name": project.project_name,
                "project_code": project.project_code,
                "epc_manager": project.epc_manager,  # é¡¹ç›®ç»ç†
                "entrust_manager": project.entrust_manager  # é¡¹ç›®æ‰§è¡Œç»ç†
            },
            "task_statistics": {
                "total_tasks": total_tasks,
                "running_tasks": status_counts['evaluating'] + status_counts['syncing'],
                "completed_tasks": completed_tasks,
                "error_tasks": status_counts['error'],
                "success_rate": round(success_rate, 1),
                "status_distribution": status_counts
            },
            "file_statistics": {
                "total_files": file_count
            },
            "recent_activity": {
                "last_task_id": recent_task.task_id if recent_task and recent_task.task_id else "é»˜è®¤ä»»åŠ¡",
                "last_task_status": recent_task.status if recent_task else 'IDLE',
                "last_update": recent_task.updated_at.strftime('%Y-%m-%d %H:%M:%S') if recent_task else None
            },
            "time_range": {
                "first_task_created": first_task.created_at.strftime('%Y-%m-%d %H:%M:%S') if first_task else None,
                "project_duration_days": (datetime.now() - first_task.created_at).days if first_task and first_task.created_at else 0
            }
        }

        logger.info(f"æˆåŠŸè·å–é¡¹ç›®ç»Ÿè®¡ä¿¡æ¯: project_id={project_id}, total_tasks={total_tasks}")

        return jsonify({
            "code": 0,
            "message": "è·å–é¡¹ç›®ç»Ÿè®¡ä¿¡æ¯æˆåŠŸ",
            "data": stats_data
        })

    except Exception as e:
        logger.error(f"è·å–é¡¹ç›®ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {str(e)}")
        return jsonify({
            "code": 500,
            "message": f"è·å–é¡¹ç›®ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {str(e)}",
            "data": None
        }), 500


if __name__ == '__main__':
    # 1. è®¾ç½®ä¼˜é›…å…³é—­å¤„ç†å™¨
    graceful_shutdown_manager.setup_signal_handlers()

    # 2. æ³¨å†Œæ¸…ç†å¤„ç†å™¨
    def cleanup_resources():
        """èµ„æºæ¸…ç†å¤„ç†å™¨"""
        logger.info("æ‰§è¡Œèµ„æºæ¸…ç†...")
        # æ¸…ç†çº¿ç¨‹èµ„æº
        thread_manager.cleanup_thread_resources()

    graceful_shutdown_manager.register_cleanup_handler(cleanup_resources)

    # 3. æ‰§è¡Œæ™ºèƒ½ä»»åŠ¡æ¢å¤
    reset_stuck_tasks()

    # 4. åˆå§‹åŒ–å¹¶å¯åŠ¨ä»»åŠ¡ç›‘æ§å™¨
    task_monitor = TaskMonitor()
    task_monitor.start_monitoring()

    print("\nğŸš€ åç«¯æœåŠ¡ V6.1 (ä¼ä¸šçº§ç¨³å®šæ€§ç‰ˆ) å¯åŠ¨")
    print("ğŸ“Š ä»»åŠ¡ç›‘æ§å™¨å·²å¯åŠ¨ - è‡ªåŠ¨æ£€æµ‹å’Œæ¢å¤å¡ä½ä»»åŠ¡")
    print("ğŸ”§ æ™ºèƒ½ä»»åŠ¡æ¢å¤å·²å¯ç”¨ - åŸºäºæ—¶é—´ç­–ç•¥çš„ä»»åŠ¡æ¢å¤")
    print("âš™ï¸ ä»»åŠ¡çŠ¶æ€æœºå·²æ¿€æ´» - ä¸¥æ ¼çš„çŠ¶æ€è½¬æ¢ç®¡ç†")
    print("â° è¶…æ—¶æ§åˆ¶å·²å¼€å¯ - è‡ªåŠ¨å¤„ç†é•¿æ—¶é—´è¿è¡Œçš„ä»»åŠ¡")
    print("ğŸ”’ çº¿ç¨‹å®‰å…¨ç®¡ç†å·²å¯ç”¨ - å¤šçº¿ç¨‹ç¯å¢ƒæ•°æ®ä¸€è‡´æ€§ä¿éšœ")
    print("ğŸ’¾ äº‹åŠ¡ç®¡ç†ä¼˜åŒ–å·²å¯ç”¨ - è‡ªåŠ¨é‡è¯•å’ŒåŸå­æ€§æ“ä½œ")
    print("ğŸ›¡ï¸ ä¼˜é›…å…³é—­æœºåˆ¶å·²æ¿€æ´» - ä¿¡å·å¤„ç†å’Œèµ„æºæ¸…ç†")

    # 5. å¯åŠ¨ Flask æœåŠ¡
    CORS(app,
     resources={r"/api/*": {"origins": "*", "methods": ["GET", "POST", "PUT", "DELETE", "OPTIONS"], "allow_headers": "*"}},
     supports_credentials=True)

    try:
        app.run(host=config.FLASK_HOST, port=config.FLASK_PORT, debug=config.FLASK_DEBUG, use_reloader=False)
    except KeyboardInterrupt:
        print("\nğŸ›‘ æ”¶åˆ°ä¸­æ–­ä¿¡å·ï¼Œå¼€å§‹ä¼˜é›…å…³é—­...")
        graceful_shutdown_manager.graceful_shutdown()
        print("âœ… æœåŠ¡å·²å®‰å…¨åœæ­¢")
    except Exception as e:
        print(f"\nâŒ æœåŠ¡å¯åŠ¨å¤±è´¥: {e}")
        graceful_shutdown_manager.graceful_shutdown()
        raise